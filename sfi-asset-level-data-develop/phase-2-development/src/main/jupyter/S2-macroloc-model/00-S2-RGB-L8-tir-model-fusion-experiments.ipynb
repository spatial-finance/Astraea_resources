{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Macro-Localization Model Fusion (TIR Landsat 8 and Sentimel 2 RGB Chips)\n",
    "This notebook explores model fusion for cement/steel/landcover classification based on [fastai](https://github.com/fastai/fastai)-trained CNNs.\n",
    "\n",
    "## Prerequisites:\n",
    "* Execute tir-macroloc-model/09-L8-TIR-model-mixup-BatchNorm.ipynb to download L8 chips and train model\n",
    "* Execute S2-macroloc-model/09-S2-RGB-model-mixup-BatchNorm.ipynb to download S2 chips and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies, including fastai\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r ../tir-macroloc-model/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "# Widget for class confusion\n",
    "from fastai.widgets import ClassConfusion\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_L8 = '/scratch/l8_macrolocalization_model/'\n",
    "SOURCE_PATH_S2 = '/scratch/s2_macrolocalization_model/'\n",
    "TARGET_PATH = '/scratch/combined_macrolocalization_model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy imagery for which we have both S2 and L8 chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_l8 = !find {SOURCE_PATH_L8}/cement/ {SOURCE_PATH_L8}/steel {SOURCE_PATH_L8}/landcover | grep png$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_l8 = !find {SOURCE_PATH_L8}/cement/ {SOURCE_PATH_L8}/steel {SOURCE_PATH_L8}/landcover | grep png$\n",
    "images_s2 = !find {SOURCE_PATH_S2}/cement/ {SOURCE_PATH_S2}/steel {SOURCE_PATH_L8}/landcover | grep png$\n",
    "\n",
    "plant_ids_l8 = ['_'.join(image.split('/')[-1].split('_')[0:2]) for image in images_l8]\n",
    "plant_ids_s2 = ['_'.join(image.split('/')[-1].split('_')[0:2]) for image in images_s2]\n",
    "\n",
    "images_l8 = dict(zip(plant_ids_l8, images_l8))\n",
    "images_s2 = dict(zip(plant_ids_s2, images_s2))\n",
    "\n",
    "common_ids = sorted(set(images_l8.keys()) & set(images_s2.keys()))\n",
    "images_l8 = [images_l8[k] for k in common_ids]\n",
    "images_s2 = [images_s2[k] for k in common_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TARGET_PATH}/l8 {TARGET_PATH}/s2\n",
    "for d in 'steel', 'cement', 'landcover':\n",
    "    for source in 's2', 'l8':\n",
    "        !mkdir -p {TARGET_PATH}/{source}/train/{d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_list, source in zip((images_l8, images_s2), ('l8', 's2')):\n",
    "    for image in image_list:\n",
    "        for c in 'steel', 'cement', 'landcover':\n",
    "            if c in image:\n",
    "                !cp {image} {TARGET_PATH}/{source}/train/{c}        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds\n",
    "Set random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data Using Fastai\n",
    "Read in image files. Partition using fixed random seed for reprodicibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for source in 'l8', 's2':\n",
    "    data[source] = ImageDataBunch.from_folder(TARGET_PATH + '/' + source, bs=16, num_workers=0, seed=42).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_model = load_learner(path='.', file='../tir-macroloc-model/vgg_final.pkl')\n",
    "s2_model = load_learner(path='.', file='../S2-macroloc-model/densenet_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Prediction Scores Using Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(data):\n",
    "    preds = {}\n",
    "    actual = {}\n",
    "    \n",
    "    preds['l8'] = np.array([np.array(l8_model.predict(im)[2]) for im in data['l8'].x])\n",
    "    preds['s2'] = np.array([np.array(s2_model.predict(im)[2]) for im in data['s2'].x])\n",
    "    \n",
    "    actual['l8'] = np.array([str(y) for y in data['l8'].y])\n",
    "    actual['s2'] = np.array([str(y) for y in data['s2'].y])\n",
    "    \n",
    "    return preds, actual\n",
    "\n",
    "preds, actual = get_preds(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(actual['l8'], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression Model on Prediction Scores and Evaluate for ((S2), (L8), (S2,L8)) Model Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=0)\n",
    "    \n",
    "    regularisation_strengths = np.logspace(-20, 20, 100)\n",
    "    penalty = 'l2'\n",
    "    model = sklearn.linear_model.LogisticRegression(penalty='l2', random_state=0, solver='liblinear', fit_intercept=False)\n",
    "    \n",
    "    pipeline = sklearn.pipeline.make_pipeline(model)\n",
    "\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for reg_strength in regularisation_strengths:\n",
    "        model.set_params(C=1/reg_strength)\n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        train_results.append(sklearn.metrics.log_loss(Y_train, pipeline.predict_proba(X_train)))\n",
    "        test_results.append(sklearn.metrics.log_loss(Y_test, pipeline.predict_proba(X_test)))\n",
    "\n",
    "    i_optim = np.argmin(test_results)\n",
    "    regularisation_strength_optim = regularisation_strengths[i_optim]\n",
    "\n",
    "    # Estimate final model using optimal regularisation parameter value\n",
    "    model.set_params(C=1/regularisation_strength_optim)\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    \n",
    "    final_results = {}\n",
    "    final_results['accuracy'] = pipeline.score(X_test, Y_test)\n",
    "    final_results['fpr'], final_results['tpr'], final_results['thresh'] = sklearn.metrics.roc_curve(Y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "    final_results['auc'] = sklearn.metrics.auc(final_results['fpr'], final_results['tpr'])\n",
    "    final_results['precision'], final_results['recall'], final_results['fbeta'], _ = sklearn.metrics.precision_recall_fscore_support(Y_test, pipeline.predict_proba(X_test)[:, 1] > 0.5)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.ylim([0.0, 0.1])\n",
    "    plt.semilogx(regularisation_strengths, np.array(train_results), label='Training data')\n",
    "    plt.semilogx(regularisation_strengths, np.array(test_results), label='Testing data')\n",
    "    plt.vlines(regularisation_strength_optim, plt.ylim()[0], plt.ylim()[1], color='k',\n",
    "               linewidth=3, label='$\\lambda_{opt}$ (testing data)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Regularisation strength $\\lambda$')\n",
    "    plt.ylabel('Cross entropy (bits)')\n",
    "    plt.title('Effect of {} regularisation strength on \\nin-sample and out-of-sample performance'.format(penalty))\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(final_results['fpr'], final_results['tpr'], color='darkorange',\n",
    "             linewidth=3, label='ROC curve (area under curve= %0.2f)' % final_results['auc'])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', linewidth=3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('Out-of-sample receiver operating characteristic\\n based on optimal {} regularisation strength'.format(penalty))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2 Model -- Steel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results = train_model(preds['s2'], actual['s2'] == 'steel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results = train_model(preds['l8'], actual['l8'] == 'steel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results = train_model(np.hstack((preds['l8'], preds['s2'])), actual['l8'] == 'steel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('steel_macrolocisation_model_fusion.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results = train_model(preds['s2'], actual['s2'] == 'cement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results = train_model(preds['l8'], actual['l8'] == 'cement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, penalty, regularisation_strengths, regularisation_strength_optim, train_results, test_results, final_results = train_model(np.hstack((preds['l8'], preds['s2'])), actual['l8'] == 'cement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cement_macrolocisation_model_fusion.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Conclusions\n",
    "For the tasks of detecting steel plants and detecting cement plants, using S2 and L8 model scores in combination yields a lower cross entropy against testing data, compared to using S2 or L8 model scores alone. On the other hand, we observe no improvement in terms of ROC AUC; for both tasks, observed model performance is already maximal for the case where we use S2 imagery alone, due to small sample sizes. The model fusion results should therefore be treated with some caution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
