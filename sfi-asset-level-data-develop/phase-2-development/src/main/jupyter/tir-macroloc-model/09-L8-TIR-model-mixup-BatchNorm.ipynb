{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Macro-Localization Model on TIR Landsat 8 Chips for Steel/Cement/Land Cover Classification\n",
    "This notebook uses the [fastai](https://github.com/fastai/fastai) library to adapt pre-trained CNNs to classify Landsat 8 TIR Band 10 image chips stored on AWS/S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies, including fastai\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r ../tir-macroloc-model/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "# Widget for class confusion\n",
    "from fastai.widgets import ClassConfusion\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download .tar Files From S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "AWS_SOURCE_PATH = 'tir-macroloc-model/'\n",
    "\n",
    "TARGET_PATH = '/scratch/l8_macrolocalization_model'\n",
    "\n",
    "IMG_DIRS = (\n",
    "    ('ALD_L8_TIR_landcover_chips_v4_B10_201801_201701_201704', 'landcover'),\n",
    "    ('ALD_L8_TIR_cement_chips_v4_B10_201801_201701_201704', 'cement'),\n",
    "    ('ALD_L8_TIR_steel_chips_v4_B10_201801_201701_201704', 'steel'),\n",
    ")\n",
    "\n",
    "!mkdir -p {TARGET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')\n",
    "\n",
    "for source_file, _ in IMG_DIRS:\n",
    "    bucket.download_file(str(Path(AWS_SOURCE_PATH, source_file + '.tar')), str(Path(TARGET_PATH, source_file + '.tar')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Contents of .tar Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_file, _ in IMG_DIRS:\n",
    "    !cd {TARGET_PATH} && tar xf {str(Path(TARGET_PATH, source_file + '.tar'))} --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GeoTiff to PNG (includes normalisation step)\n",
    "Fastai appears to require converting TIFF files to an alternative image format. Thus, convert from GeoTIFF to PNG. The step of normalizing the resulting images is necessary for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, lower=0, upper=65535):\n",
    "    \"\"\"Stretch the max value to nan max and min to nan min\"\"\"\n",
    "    x_max = np.nanmax(x, axis=(1, 2), keepdims=True)\n",
    "    x_min = np.nanmin(x, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    m = (upper - lower) / (x_max - x_min)\n",
    "    x_norm = (m * (x - x_min)) + lower\n",
    "\n",
    "    return x_norm.astype(\"uint16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each image only if its corresponding target file does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_in_dir(input_tif_dir, output_png_dir, normalize=normalize):\n",
    "    def convert_image(tif_filename):\n",
    "        with rasterio.open(Path(input_tif_dir, tif_filename)) as infile:\n",
    "            profile = infile.profile\n",
    "            profile['driver'] = 'PNG'\n",
    "            \n",
    "            png_filename = Path(tif_filename).with_suffix('.png')\n",
    "            raster = infile.read()\n",
    "            raster = normalize(raster)\n",
    "\n",
    "            with rasterio.open(Path(output_png_dir, png_filename), 'w', **profile) as dst:\n",
    "                dst.write(raster)\n",
    "    \n",
    "    output_png_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for f in os.listdir(input_tif_dir):\n",
    "        if f.endswith('.tif') and not Path(output_png_dir, f).with_suffix('.png').is_file():\n",
    "            convert_image(f)\n",
    "\n",
    "for input_dir, output_dir in IMG_DIRS:\n",
    "    convert_in_dir(Path(TARGET_PATH, input_dir), Path(TARGET_PATH, output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition the Data Using Stratified Random Sampling\n",
    "To help address the issue of limited sample sizes (in particular for steel plant imagery), we partitition the data using stratified random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = ! find {TARGET_PATH} | grep png$\n",
    "class_assignments = [f.split('/')[-2] for f in image_list]\n",
    "\n",
    "train_idx, val_idx = next(sklearn.model_selection.StratifiedShuffleSplit(n_splits=2, random_state=42, test_size=0.2).split(class_assignments, class_assignments))\n",
    "subset_assignments = ['train' if i in train_idx else 'validate' for i in range(len(image_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_class in np.unique(class_assignments):\n",
    "    for subset in np.unique(subset_assignments):\n",
    "        !mkdir -p {TARGET_PATH}/{subset}/{image_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_file, class_assignment, subset_assignment in zip(image_list, class_assignments, subset_assignments):\n",
    "    if not Path(TARGET_PATH, subset_assignment, class_assignment, image_file.split('/')[-1]).exists():\n",
    "        !ln -s {image_file} {TARGET_PATH}/{subset_assignment}/{class_assignment}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds\n",
    "Set random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in, Augment and Partition Image Data\n",
    "Read in image files and augment them using flipping, rotation, zoom, lighting, warping, and affine transformations. Partition using fixed random seed for reprodicibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True,\n",
    "                      flip_vert=True, \n",
    "                      max_lighting=None, \n",
    "                      max_zoom=1.5, \n",
    "                      max_warp=0.2)\n",
    "\n",
    "data = (ImageDataBunch.from_folder(TARGET_PATH, valid='validate', ds_tfms=tfms, bs=16, num_workers=0, seed=42)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display class-wise counts for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classwise_counts(items, classes):\n",
    "    series = pd.value_counts(items).sort_index()\n",
    "    series.index = classes\n",
    "    \n",
    "    return series\n",
    "\n",
    "for subset, label in zip((data.train_ds, data.valid_ds), ('Training set', 'Validation set')):\n",
    "    print('--- {} ---'.format(label))\n",
    "    print(get_classwise_counts(subset.y.items, subset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exploratory purposes, display a sample of images from a single training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=4, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1 - Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt Resnet50 using a weighted cross entropy as a custom loss function and using mixup to train the model. In addition, we will optimise models for recall, by selecting among training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "interpretations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1, 1, 1]\n",
    "# Replace the weight for the most abundant class with a smaller value\n",
    "weights[np.argmax(get_classwise_counts(data.train_ds.y.items, data.train_ds.classes).values)] = 0.2\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "loss_w = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate metrics\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "# fbeta = MultiLabelFbeta(beta =1)\n",
    "fbeta = FBeta()\n",
    "metrics_all = [accuracy, recall, precision, fbeta]\n",
    "metrics_labels = ['Accuracy', 'Recall', 'Precision', 'Fbeta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(data, \n",
    "                      models.resnet50,\n",
    "                      # transfer learning on\n",
    "                      pretrained=True,\n",
    "                      # loss_func = LabelSmoothingCrossEntropy(), \n",
    "                      # class weighted cross entropy loss\n",
    "                      loss_func=loss_w,\n",
    "                      metrics=metrics_all,\n",
    "                      opt_func=optim.Adam,\n",
    "                      # batch norm at the end of the CNN\n",
    "                      bn_final=True,\n",
    "                      # nice callback for plotting loss for training and \n",
    "                      # validation during fitting \n",
    "                      # followed by mixup\n",
    "                      callback_fns=ShowGraph).mixup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the learning rate based on Smith's (2015) range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_learning_rate(learner, show_plot=True):\n",
    "    learner.lr_find()\n",
    "    if show_plot:\n",
    "        learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_recall_optimised(learner, n_epochs, max_learning_rate, model_filename):\n",
    "    learner.fit_one_cycle(n_epochs, max_learning_rate,\n",
    "                          callbacks=[callbacks.SaveModelCallback(learner, every='improvement', monitor='recall', name=model_filename)])\n",
    "\n",
    "    learner.recorder.plot_losses() #, learner.recorder.plot_metrics()\n",
    "    interpretation = ClassificationInterpretation.from_learner(learner)\n",
    "    interpretation.plot_confusion_matrix(title='Confusion matrix', dpi=100)\n",
    "    \n",
    "    return interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(learner):\n",
    "    return dict(zip(metrics_labels, np.array(learner.validate(metrics=metrics_all))[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, a learning rate of 1E-02 appears to be reasonable, owing to the magnitude and slope of the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=25, max_learning_rate=1e-02, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the entire model. We perform this by unfreezing the model, then repeating the learning rate range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the best recall\n",
    "learner.load('resnet_temp')\n",
    "learner.unfreeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, further train the model using a learning rate of 10E-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=1e-04, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best recall-optimised model, freeze and re-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('resnet_temp')\n",
    "learner.freeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=7e-04, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, load the best recall-optimised model, unfreeze and re-train using a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('resnet_temp')\n",
    "learner.unfreeze()\n",
    "interpretations['resnet'] = fit_recall_optimised(learner, n_epochs=15, max_learning_rate=1e-06, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(str(Path(CURRENT_DIRECTORY, 'resnet_final.pkl')))\n",
    "results['resnet'] = get_statistics(learner)\n",
    "results['resnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, actual = learner.get_preds(ds_type=DatasetType.Train)\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2 - VGG13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt VGG13 using a weighted cross entropy as a custom loss function and using mixup to train the model. In addition, we will optimise models for recall, by selecting among training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(data, \n",
    "                      models.vgg13_bn,\n",
    "                      # transfer learning on\n",
    "                      pretrained=True,\n",
    "                      # loss_func = LabelSmoothingCrossEntropy(), \n",
    "                      # class weighted cross entropy loss\n",
    "                      loss_func=loss_w,\n",
    "                      metrics=metrics_all,\n",
    "                      opt_func=optim.Adam,\n",
    "                      # batch norm at the end of the CNN\n",
    "                      bn_final=True,\n",
    "                      # nice callback for plotting loss for training and \n",
    "                      # validation during fitting \n",
    "                      # followed by mixup\n",
    "                      callback_fns=ShowGraph).mixup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the learning rate based on Smith's (2015) range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, a learning rate of 1E-02 appears to be reasonable, owing to the magnitude and slope of the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=25, max_learning_rate=1e-02, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the entire model. We perform this by unfreezing the model, then repeating the learning rate range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the best recall\n",
    "learner.load('vgg_temp')\n",
    "learner.unfreeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, further train the model using a learning rate of 10E-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=1e-04, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best recall-optimised model, freeze and re-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('vgg_temp')\n",
    "learner.freeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=7e-04, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, load the best recall-optimised model, unfreeze and re-train using a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('vgg_temp')\n",
    "learner.unfreeze()\n",
    "interpretations['vgg'] = fit_recall_optimised(learner, n_epochs=15, max_learning_rate=1e-06, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(str(Path(CURRENT_DIRECTORY, 'vgg_final.pkl')))\n",
    "results['vgg'] = get_statistics(learner)\n",
    "results['vgg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3 - Densenet161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt Densenet161 using a weighted cross entropy as a custom loss function and using mixup to train the model. In addition, we will optimise models for recall, by selecting among training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(data, \n",
    "                      models.densenet161,\n",
    "                      # transfer learning on\n",
    "                      pretrained=True,\n",
    "                      # loss_func = LabelSmoothingCrossEntropy(), \n",
    "                      # class weighted cross entropy loss\n",
    "                      loss_func=loss_w,\n",
    "                      metrics=metrics_all,\n",
    "                      opt_func=optim.Adam,\n",
    "                      # batch norm at the end of the CNN\n",
    "                      bn_final=True,\n",
    "                      # nice callback for plotting loss for training and \n",
    "                      # validation during fitting \n",
    "                      # followed by mixup\n",
    "                      callback_fns=ShowGraph).mixup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the learning rate based on Smith's (2015) range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, a learning rate of 1E-02 appears to be reasonable, owing to the magnitude and slope of the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=25, max_learning_rate=1e-02, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the entire model. We perform this by unfreezing the model, then repeating the learning rate range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the best recall\n",
    "learner.load('densenet_temp')\n",
    "learner.unfreeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, further train the model using a learning rate of 10E-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=1e-04, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best recall-optimised model, freeze and re-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('densenet_temp')\n",
    "learner.freeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=7e-04, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, load the best recall-optimised model, unfreeze and re-train using a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('densenet_temp')\n",
    "learner.unfreeze()\n",
    "interpretations['densenet'] = fit_recall_optimised(learner, n_epochs=15, max_learning_rate=1e-06, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(str(Path(CURRENT_DIRECTORY, 'densenet_final.pkl')))\n",
    "results['densenet'] = get_statistics(learner)\n",
    "results['densenet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Summary of Results Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on obtained results, we select Resnet as the best-performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Results Obtained Using VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassConfusion(interpretations['vgg'], classlist=['cement','landcover','steel'], is_ordered=False, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of largest non-diagonal entries in the confusion matrix (actual | predicted | number of occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretations['vgg'].most_confused()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
