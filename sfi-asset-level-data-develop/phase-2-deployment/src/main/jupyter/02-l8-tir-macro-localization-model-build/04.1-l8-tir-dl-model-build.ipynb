{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Landsat-8 TIR Macro-localization Deep Learning Model\n",
    "\n",
    "This notebook trains models to classify Landsat 8 TIR Band 10 image chips into cement, steel, and landcover.\n",
    "\n",
    "TBD more...\n",
    "\n",
    "## Install Earlier Version of fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==1.0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.widgets import ClassConfusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download .tar File From S3 Bucket and Extract Contents\n",
    "\n",
    "This tar file contains normalized PNGs for cement, steel, and landcover, divided into train and validate sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_SOURCE_PATH = 'L8-TIR-macro-localization-model-build3'\n",
    "IMG_DIR = '/scratch/ALD_L8_TIR_chips_v4p1_train3'\n",
    "\n",
    "# Output\n",
    "AWS_MODEL_PATH = 'L8-TIR-model-results3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')\n",
    "\n",
    "bucket.download_file(str(Path(AWS_SOURCE_PATH, IMG_DIR.split('/')[-1]+'.tar')), \n",
    "                     IMG_DIR+'.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -C /scratch/ -xf '+IMG_DIR+'.tar'\n",
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds\n",
    "Set random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in, Augment and Partition Image Data\n",
    "Read in image files and augment them using flipping, rotation, zoom, lighting, warping, and affine transformations. Partition using fixed random seed for reprodicibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=True,\n",
    "                      flip_vert=True, \n",
    "                      max_lighting=None, \n",
    "                      max_zoom=1.5, \n",
    "                      max_warp=0.2)\n",
    "\n",
    "data = (ImageDataBunch.from_folder(IMG_DIR, train='train', valid='validate', \n",
    "                                   ds_tfms=tfms, bs=16, num_workers=0, seed=42)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display class-wise counts for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classwise_counts(items, classes):\n",
    "    series = pd.value_counts(items).sort_index()\n",
    "    series.index = classes\n",
    "    \n",
    "    return series\n",
    "\n",
    "for subset, label in zip((data.train_ds, data.valid_ds), ('Training set', 'Validation set')):\n",
    "    print('--- {} ---'.format(label))\n",
    "    print(get_classwise_counts(subset.y.items, subset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exploratory purposes, display a sample of images from a single training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=4, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Setup for Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "interpretations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1, 1, 1]\n",
    "# Replace the weight for the most abundant class with a smaller value\n",
    "weights[np.argmax(get_classwise_counts(data.train_ds.y.items, data.train_ds.classes).values)] = 0.2\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "loss_w = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate metrics\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "# fbeta = MultiLabelFbeta(beta =1)\n",
    "fbeta = FBeta()\n",
    "metrics_all = [accuracy, recall, precision, fbeta]\n",
    "metrics_labels = ['Accuracy', 'Recall', 'Precision', 'Fbeta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Tune Learning Rate\n",
    "\n",
    "Tunes the learning rate based on Smith's (2015) range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_learning_rate(learner, show_plot=True):\n",
    "    learner.lr_find()\n",
    "    if show_plot:\n",
    "        learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_recall_optimised(learner, n_epochs, max_learning_rate, model_filename):\n",
    "    learner.fit_one_cycle(n_epochs, max_learning_rate,\n",
    "                          callbacks=[callbacks.SaveModelCallback(learner, every='improvement', monitor='recall', name=model_filename)])\n",
    "\n",
    "    learner.recorder.plot_losses() #, learner.recorder.plot_metrics()\n",
    "    interpretation = ClassificationInterpretation.from_learner(learner)\n",
    "    interpretation.plot_confusion_matrix(title='Confusion matrix', dpi=100)\n",
    "    \n",
    "    return interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(learner):\n",
    "    return dict(zip(metrics_labels, np.array(learner.validate(metrics=metrics_all))[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 1 - Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt Resnet50 using a weighted cross entropy as a custom loss function and using mixup to train the model. In addition, we will optimise models for recall, by selecting among training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Learner (Resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(data, \n",
    "                      models.resnet50,\n",
    "                      # transfer learning on\n",
    "                      pretrained=True,\n",
    "                      # loss_func = LabelSmoothingCrossEntropy(), \n",
    "                      # class weighted cross entropy loss\n",
    "                      loss_func=loss_w,\n",
    "                      metrics=metrics_all,\n",
    "                      opt_func=optim.Adam,\n",
    "                      # batch norm at the end of the CNN\n",
    "                      bn_final=True,\n",
    "                      # nice callback for plotting loss for training and \n",
    "                      # validation during fitting \n",
    "                      # followed by mixup\n",
    "                      callback_fns=ShowGraph).mixup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Learning Rate (Resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Resnet50 with initial (high) learning rate\n",
    "\n",
    "Based on the range test, a learning rate of 1E-02 appears to be reasonable, owing to the magnitude and slope of the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=25, max_learning_rate=1e-02, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine new learning rate (Resnet50)\n",
    "\n",
    "Fine tune the entire model. We perform this by unfreezing the model, then repeating the learning rate range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the best recall\n",
    "learner.load('resnet_temp')\n",
    "learner.unfreeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Resnet50 with low learning rate\n",
    "\n",
    "Based on the range test, further train the model using a learning rate of 10E-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=1e-04, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Resnet50 from best recall-optimized model\n",
    "\n",
    "Load the best recall-optimised model, freeze and re-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('resnet_temp')\n",
    "learner.freeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=7e-04, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Resnet50 training\n",
    "\n",
    "As a final step, load the best recall-optimised model, unfreeze and re-train using a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('resnet_temp')\n",
    "learner.unfreeze()\n",
    "interpretations['resnet'] = fit_recall_optimised(learner, n_epochs=15, max_learning_rate=1e-06, model_filename='resnet_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export('resnet50_multiclass_final.pkl')\n",
    "#results['resnet'] = get_statistics(learner)\n",
    "#results['resnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, actual = learner.get_preds(ds_type=DatasetType.Train)\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 2 - VGG13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt VGG13 using a weighted cross entropy as a custom loss function and using mixup to train the model. In addition, we will optimise models for recall, by selecting among training epochs.\n",
    "\n",
    "### Define Learner (VGG13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(data, \n",
    "                      models.vgg13_bn,\n",
    "                      # transfer learning on\n",
    "                      pretrained=True,\n",
    "                      # loss_func = LabelSmoothingCrossEntropy(), \n",
    "                      # class weighted cross entropy loss\n",
    "                      loss_func=loss_w,\n",
    "                      metrics=metrics_all,\n",
    "                      opt_func=optim.Adam,\n",
    "                      # batch norm at the end of the CNN\n",
    "                      bn_final=True,\n",
    "                      # nice callback for plotting loss for training and \n",
    "                      # validation during fitting \n",
    "                      # followed by mixup\n",
    "                      callback_fns=ShowGraph).mixup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Learning Rate (VGG13)\n",
    "\n",
    "Tune the learning rate based on Smith's (2015) range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Round of VGG13 Training with High Learning Rate\n",
    "\n",
    "Based on the range test, a learning rate of 1E-02 appears to be reasonable, owing to the magnitude and slope of the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=25, max_learning_rate=1e-02, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat Learning Rate Test and Retrain (VGG13)\n",
    "\n",
    "Fine tune the entire model. We perform this by unfreezing the model, then repeating the learning rate range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the best recall\n",
    "learner.load('vgg_temp')\n",
    "learner.unfreeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, further train the model using a learning rate of 10E-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=1e-04, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain from recall-optimiated model (VGG13)\n",
    "\n",
    "Load the best recall-optimised model, freeze and re-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('vgg_temp')\n",
    "learner.freeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=7e-04, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training of VGG13 with low learning rate\n",
    "\n",
    "As a final step, load the best recall-optimised model, unfreeze and re-train using a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('vgg_temp')\n",
    "learner.unfreeze()\n",
    "interpretations['vgg'] = fit_recall_optimised(learner, n_epochs=15, max_learning_rate=1e-06, model_filename='vgg_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export('vgg13_multiclass_final.pkl')\n",
    "results['vgg'] = get_statistics(learner)\n",
    "results['vgg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 3 - Densenet161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Learner (Densenet161)\n",
    "\n",
    "Adapt Densenet161 using a weighted cross entropy as a custom loss function and using mixup to train the model. In addition, we will optimise models for recall, by selecting among training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = cnn_learner(data, \n",
    "                      models.densenet161,\n",
    "                      # transfer learning on\n",
    "                      pretrained=True,\n",
    "                      # loss_func = LabelSmoothingCrossEntropy(), \n",
    "                      # class weighted cross entropy loss\n",
    "                      loss_func=loss_w,\n",
    "                      metrics=metrics_all,\n",
    "                      opt_func=optim.Adam,\n",
    "                      # batch norm at the end of the CNN\n",
    "                      bn_final=True,\n",
    "                      # nice callback for plotting loss for training and \n",
    "                      # validation during fitting \n",
    "                      # followed by mixup\n",
    "                      callback_fns=ShowGraph).mixup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Learning Rate (Densenet161)\n",
    "\n",
    "Tune the learning rate based on Smith's (2015) range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, a learning rate of 1E-02 appears to be reasonable, owing to the magnitude and slope of the associated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=25, max_learning_rate=1e-02, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Densenet161 with lower learning rate\n",
    "\n",
    "Fine tune the entire model. We perform this by unfreezing the model, then repeating the learning rate range test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the best recall\n",
    "learner.load('densenet_temp')\n",
    "learner.unfreeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the range test, further train the model using a learning rate of 10E-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=1e-04, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain recall-optimized model (Densenet161)\n",
    "\n",
    "Load the best recall-optimised model, freeze and re-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('densenet_temp')\n",
    "learner.freeze()\n",
    "find_learning_rate(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_recall_optimised(learner, n_epochs=10, max_learning_rate=7e-04, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training of Densenet161 with low learning rate\n",
    "\n",
    "As a final step, load the best recall-optimised model, unfreeze and re-train using a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('densenet_temp')\n",
    "learner.unfreeze()\n",
    "interpretations['densenet'] = fit_recall_optimised(learner, n_epochs=15, max_learning_rate=1e-06, model_filename='densenet_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export('densenet161_multiclass_final.pkl')\n",
    "results['densenet'] = get_statistics(learner)\n",
    "results['densenet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Summary of Results Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on obtained results, we select Resnet as the best-performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results Obtained Using VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassConfusion(interpretations['vgg'], classlist=['cement','landcover','steel'], is_ordered=False, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of largest non-diagonal entries in the confusion matrix (actual | predicted | number of occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretations['vgg'].most_confused()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload models to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = glob.glob(IMG_DIR+'/*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model_results:\n",
    "    bucket.upload_file(m, AWS_SOURCE_PATH+'/'+AWS_MODEL_PATH+'/'+m.split('/')[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
