{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of TIR Landsat 8 Chips for Model Build\n",
    "\n",
    "This notebook creates the Landsat 8 TIR Band 10 image chips for cement, steel, or landcover sites from the EarthAI catalog.\n",
    "\n",
    "For final model build, we created:\n",
    "\n",
    "1. 4 sets of steel plant chips for years 2020, 2019, 2018, and 2017\n",
    "2. 3 sets of cement plant chips for years 2020, 2019, and 2018\n",
    "3. 1 set of landcover chips for year 2020\n",
    "\n",
    "Documentation on Landsat 8 L1TP:\n",
    "https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/atoms/files/LSDS-1656_%20Landsat_Collection1_L1_Product_Definition-v2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as F\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input and output files and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `site_type` should be set to `'cement'`, `'steel'`, or `'landcover'`\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `unmsk_frac` is the minimum threshold on the fraction of unmasked cells required to keep site in sample\n",
    "* `year2` defines the year for layer 1 (thermal band, in January)\n",
    "* `year1` defines the year for layers 2 and 3, (thermal band, in January and April, respectively)\n",
    "\n",
    "The original model was hardcoded for `year2 = '2018'` and `year1 = '2017'`; without more testing it is recommended that `year2 = [year1 + 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_type = 'landcover'\n",
    "\n",
    "chip_size = 35 # 1.05 km for Landsat 8\n",
    "unmsk_frac = 0.75\n",
    "\n",
    "year2 = '2018'\n",
    "year1 = '2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files\n",
    "\n",
    "* `site_geojson` is a GeoJSON specifying the site locations (Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if site_type == 'landcover':\n",
    "    gjson_prefix = 'lc'\n",
    "else:\n",
    "    gjson_prefix = site_type\n",
    "site_geojson = \"../../resources/macro-loc-model-build/\"+gjson_prefix+\"_exact_china_v4.1.geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `output_path` defines directory to write chip GeoTIFFs to, and sub-folder on S3 where chips are stored\n",
    "* `s3_path` defines S3 high-level folder for L8 TIR macro-localization data\n",
    "* `filename_append` is appended to each chip file name\n",
    "* `chip_extents_gjson` is an output GeoJSON file with chip metadata and tile extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'ALD_L8_TIR_'+site_type+'_chips_v4p1_'+year2+'_train3'\n",
    "s3_path = 'L8-TIR-macro-localization-model-build3'\n",
    "\n",
    "filename_append = 'v4p1_'+year2+'_L8_TIR'\n",
    "chip_extents_gjson = '../../resources/macro-loc-model-build/'+output_path+'.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load site location point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_gdf = gpd.read_file(site_geojson)\n",
    "print(\"Total count of sites: \", len(site_gdf))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FOR TESTING ONLY - LIMIT NUMBER OF PLANTS\n",
    "site_gdf = site_gdf.head(25)\n",
    "site_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get catalog of Landsat 8 scenes that intersect with sites\n",
    "\n",
    "* Queries EarthAI Catalog to find L8 scenes that intersect with sites\n",
    "* Returns all scenes for:\n",
    "    * January Year 2\n",
    "    * January Year 1\n",
    "    * April Year 1\n",
    "* Join back to site location data for chipping\n",
    "\n",
    "Below, we do NOT impose a maximum cloud cover filter. Since sites are small, it's possible that a high-cloud\n",
    "coverage scene is relatively clear over the small region we need. Will select thes highest quality scenes\n",
    "after the masking steps below.\n",
    "\n",
    "### January Year 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cat_year2_01 = earth_ondemand.read_catalog(\n",
    "    geo=site_gdf,\n",
    "    start_datetime=year2+'-01-01', \n",
    "    end_datetime=year2+'-01-31',\n",
    "    max_cloud_cover=100,\n",
    "    collections='landsat8_l1tp')\n",
    "site_cat_year2_01 = gpd.sjoin(site_gdf, site_cat_year2_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### January Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cat_year1_01 = earth_ondemand.read_catalog(\n",
    "    site_gdf,\n",
    "    start_datetime=year1+'-01-01', \n",
    "    end_datetime=year1+'-01-31',\n",
    "    max_cloud_cover=100,\n",
    "    collections='landsat8_l1tp'\n",
    ")\n",
    "site_cat_year1_01 = gpd.sjoin(site_gdf, site_cat_year1_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### April Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cat_year1_04 = earth_ondemand.read_catalog(\n",
    "    site_gdf,\n",
    "    start_datetime=year1+'-04-01', \n",
    "    end_datetime=year1+'-04-30',\n",
    "    max_cloud_cover=100,\n",
    "    collections='landsat8_l1tp'\n",
    ")\n",
    "site_cat_year1_04 = gpd.sjoin(site_gdf, site_cat_year1_04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark\n",
    "\n",
    "Set the number of partitions to be proportional to catalog size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = round(len(site_cat_year2_01) / 4)\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and create image chips for sites\n",
    "\n",
    "* Uses chip reader to create uniform, same-sized chips covering all sites\n",
    "* Filter out blank chips at edge of scenes\n",
    "* Handle rare edge case where returned chip is less than specified size (when reach edge of a scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### January Year 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year2_01 = spark.read.chip(site_cat_year2_01, ['BQA'],\n",
    "                                    chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                         .select('uid', 'id', 'BQA') \\\n",
    "                         .withColumn('mask', rf_make_constant_tile(1, chip_size, chip_size, 'uint16')) \\\n",
    "                         .withColumn('tot_cell_count', rf_data_cells('BQA')) \\\n",
    "                         .filter(F.col('tot_cell_count') == chip_size*chip_size) \\\n",
    "                         .withColumn('BQA_min', rf_tile_min('BQA')) \\\n",
    "                         .filter(F.col('BQA_min') > 1.0) \\\n",
    "                         .repartition(partitions, 'uid', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### January Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year1_01 = spark.read.chip(site_cat_year1_01, ['BQA'],\n",
    "                                    chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                         .select('uid', 'id', 'BQA') \\\n",
    "                         .withColumn('mask', rf_make_constant_tile(1, chip_size, chip_size, 'uint16')) \\\n",
    "                         .withColumn('tot_cell_count', rf_data_cells('BQA')) \\\n",
    "                         .filter(F.col('tot_cell_count') == chip_size*chip_size) \\\n",
    "                         .withColumn('BQA_min', rf_tile_min('BQA')) \\\n",
    "                         .filter(F.col('BQA_min') > 1.0) \\\n",
    "                         .repartition(partitions, 'uid', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### April Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year1_04 = spark.read.chip(site_cat_year1_04, ['BQA'],\n",
    "                                    chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                         .select('uid', 'id', 'BQA') \\\n",
    "                         .withColumn('mask', rf_make_constant_tile(1, chip_size, chip_size, 'uint16')) \\\n",
    "                         .withColumn('tot_cell_count', rf_data_cells('BQA')) \\\n",
    "                         .filter(F.col('tot_cell_count') == chip_size*chip_size) \\\n",
    "                         .withColumn('BQA_min', rf_tile_min('BQA')) \\\n",
    "                         .filter(F.col('BQA_min') > 1.0) \\\n",
    "                         .repartition(partitions, 'uid', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select highest quality chips per site\n",
    "\n",
    "* Mask chips by QA band and compute count of unmasked cells\n",
    "* Remove chips with less than a minimum fraction of unmasked cells\n",
    "* For each site, keep the chip with the highest number of unmasked cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask by QA band\n",
    "\n",
    "* Landsat 8 Collection 1 Tier 1 QA band description: https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1-level-1-quality-assessment-band?qt-science_support_page_related_con=0#qt-science_support_page_related_con\n",
    "* In order to apply a mask, the tile must have a NoData defined. Landsat 8 measurement bands have a cell type of uint16raw, which indicates that there is no NoData value defined. The first line of the code below sets the cell types to uint16, whose NoData value is 0. This will cause any zero-valued cells in the measurement band to be considered NoData. In Landsat 8, these areas correspond to the BQA fill areas.\n",
    "* Remove chips with less than minimum threshold of unmasked cells\n",
    "\n",
    "#### January Year 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year2_01 = site_chip_year2_01.withColumn('mask', # designated fill = yes\n",
    "                                                 rf_mask_by_bit('mask', 'BQA', 0, 1)) \\\n",
    "                                     .withColumn('mask', # cloud = yes\n",
    "                                                 rf_mask_by_bit('mask', 'BQA', 4, 1)) \\\n",
    "                                     .withColumn('mask', # cloud shadow conf is medium or high\n",
    "                                                 rf_mask_by_bits('mask', 'BQA', 7, 2, [2, 3])) \\\n",
    "                                     .withColumn('mask', # cirrus conf is medium or high\n",
    "                                                 rf_mask_by_bits('mask', 'BQA', 11, 2, [2, 3])) \\\n",
    "                                     .withColumn('unmsk_cell_count', rf_data_cells('mask')) \\\n",
    "                                     .filter(F.col('unmsk_cell_count') >= unmsk_frac*chip_size*chip_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### January Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year1_01 = site_chip_year1_01.withColumn('mask', # designated fill = yes\n",
    "                                                 rf_mask_by_bit('mask', 'BQA', 0, 1)) \\\n",
    "                                     .withColumn('mask', # cloud = yes\n",
    "                                                 rf_mask_by_bit('mask', 'BQA', 4, 1)) \\\n",
    "                                     .withColumn('mask', # cloud shadow conf is medium or high\n",
    "                                                 rf_mask_by_bits('mask', 'BQA', 7, 2, [2, 3])) \\\n",
    "                                     .withColumn('mask', # cirrus conf is medium or high\n",
    "                                                 rf_mask_by_bits('mask', 'BQA', 11, 2, [2, 3])) \\\n",
    "                                     .withColumn('unmsk_cell_count', rf_data_cells('mask')) \\\n",
    "                                     .filter(F.col('unmsk_cell_count') >= unmsk_frac*chip_size*chip_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### April Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year1_04 = site_chip_year1_04.withColumn('mask', # designated fill = yes\n",
    "                                                 rf_mask_by_bit('mask', 'BQA', 0, 1)) \\\n",
    "                                     .withColumn('mask', # cloud = yes\n",
    "                                                 rf_mask_by_bit('mask', 'BQA', 4, 1)) \\\n",
    "                                     .withColumn('mask', # cloud shadow conf is medium or high\n",
    "                                                 rf_mask_by_bits('mask', 'BQA', 7, 2, [2, 3])) \\\n",
    "                                     .withColumn('mask', # cirrus conf is medium or high\n",
    "                                                 rf_mask_by_bits('mask', 'BQA', 11, 2, [2, 3])) \\\n",
    "                                     .withColumn('unmsk_cell_count', rf_data_cells('mask')) \\\n",
    "                                     .filter(F.col('unmsk_cell_count') >= unmsk_frac*chip_size*chip_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best-quality chips\n",
    "\n",
    "For each date:\n",
    "* Find the chip(s) with the highest number of unmasked cells\n",
    "* If there's >1 chip (a tie) take the first record\n",
    "* Read in thermal band for highest quality chip\n",
    "* Normalize chips for min/max range of 0 to 65535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### January Year 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpinf_year2_01_pdf = site_chip_year2_01.select('uid', 'id', 'unmsk_cell_count').toPandas()\n",
    "chpinf_year2_01_pdf['grpid'] = chpinf_year2_01_pdf['uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_year2_01_maxcnt = chpinf_year2_01_pdf.sort_values('unmsk_cell_count', ascending=False) \\\n",
    "                                        .groupby(['grpid']).first() \\\n",
    "                                        .drop('unmsk_cell_count', axis=1)\n",
    "site_cat_year2_01 = site_cat_year2_01.merge(site_year2_01_maxcnt, on=['uid', 'id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year2_01_unq = spark.read.chip(site_cat_year2_01, ['B10'],\n",
    "                                        chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                             .select('uid', 'id', 'datetime', 'B10') \\\n",
    "                             .withColumn('B10_JY2', \n",
    "                                         rf_convert_cell_type(rf_local_multiply(rf_rescale(rf_convert_cell_type('B10', 'uint16')), \n",
    "                                                                                65535), 'uint16')) \\\n",
    "                             .drop('B10') \\\n",
    "                             .withColumnRenamed('id', 'id_JY2') \\\n",
    "                             .withColumnRenamed('datetime', 'datetime_JY2') \\\n",
    "                             .repartition(partitions, 'uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### January Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpinf_year1_01_pdf = site_chip_year1_01.select('uid', 'id', 'unmsk_cell_count').toPandas()\n",
    "chpinf_year1_01_pdf['grpid'] = chpinf_year1_01_pdf['uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_year1_01_maxcnt = chpinf_year1_01_pdf.sort_values('unmsk_cell_count', ascending=False) \\\n",
    "                                        .groupby(['grpid']).first() \\\n",
    "                                        .drop('unmsk_cell_count', axis=1)\n",
    "site_cat_year1_01 = site_cat_year1_01.merge(site_year1_01_maxcnt, on=['uid', 'id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year1_01_unq = spark.read.chip(site_cat_year1_01, ['B10'],\n",
    "                                        chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                             .select('uid', 'id', 'datetime', 'B10') \\\n",
    "                             .withColumn('B10_JY1', \n",
    "                                         rf_convert_cell_type(rf_local_multiply(rf_rescale(rf_convert_cell_type('B10', 'uint16')), \n",
    "                                                                                65535), 'uint16')) \\\n",
    "                             .drop('B10') \\\n",
    "                             .withColumnRenamed('id', 'id_JY1') \\\n",
    "                             .withColumnRenamed('datetime', 'datetime_JY1') \\\n",
    "                             .repartition(partitions, 'uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### April Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chpinf_year1_04_pdf = site_chip_year1_04.select('uid', 'id', 'unmsk_cell_count').toPandas()\n",
    "chpinf_year1_04_pdf['grpid'] = chpinf_year1_04_pdf['uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_year1_04_maxcnt = chpinf_year1_04_pdf.sort_values('unmsk_cell_count', ascending=False) \\\n",
    "                                        .groupby(['grpid']).first() \\\n",
    "                                        .drop('unmsk_cell_count', axis=1)\n",
    "site_cat_year1_04 = site_cat_year1_04.merge(site_year1_04_maxcnt, on=['uid', 'id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_year1_04_unq = spark.read.chip(site_cat_year1_04, ['B10'],\n",
    "                                        chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                             .select('uid', 'id', 'datetime', 'B10') \\\n",
    "                             .withColumn('B10_AY1', \n",
    "                                         rf_convert_cell_type(rf_local_multiply(rf_rescale(rf_convert_cell_type('B10', 'uint16')), \n",
    "                                                                                65535), 'uint16')) \\\n",
    "                             .drop('B10') \\\n",
    "                             .withColumnRenamed('id', 'id_AY1') \\\n",
    "                             .withColumnRenamed('datetime', 'datetime_AY1') \\\n",
    "                             .repartition(partitions, 'uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join TIR chips\n",
    "\n",
    "* Join TIR chips at different dates into single RasterFrame\n",
    "* Keep only sites where all three dates are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chips_joined = site_chip_year2_01_unq.join(site_chip_year1_01_unq, on=['uid'], how='inner') \\\n",
    "                                         .join(site_chip_year1_04_unq, on=['uid'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write chips out as GeoTIFFs\n",
    "\n",
    "* Writes chips to scratch directory\n",
    "* Bundles output into tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/scratch/'+output_path):\n",
    "    shutil.rmtree('/scratch/'+output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chips_joined = site_chips_joined.withColumn('file_path_name', \n",
    "                                                 F.concat_ws('_', F.col('uid'), lit(site_type), lit(filename_append))) \\\n",
    "                                     .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chips_joined.write.chip('/scratch/'+output_path, filenameCol='file_path_name', \n",
    "                             catalog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -C /scratch -cvf '+output_path+'.tar '+output_path\n",
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload tar files to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')\n",
    "\n",
    "bucket.upload_file(output_path+'.tar', \n",
    "                   s3_path+'/'+output_path+'.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out Vector File of Tile Extents and Metadata\n",
    "\n",
    "* Serves as metadata catalog for chips\n",
    "* Tile extent for just January Year 2 included for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chips_geom_pdf = site_chips_joined.withColumn('tile_extent',\n",
    "                                                   st_reproject(st_geometry(rf_extent('B10_JY2')),\n",
    "                                                                rf_crs('B10_JY2'), lit('EPSG:4326'))) \\\n",
    "                                      .drop('B10_JY2', 'B10_JY1', 'B10_AY1') \\\n",
    "                                      .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chips_geom_gdf = gpd.GeoDataFrame(site_chips_geom_pdf.drop('tile_extent', axis=1),\n",
    "                                       geometry=site_chips_geom_pdf.tile_extent,\n",
    "                                       crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chips_geom_gdf.to_file(chip_extents_gjson, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/scratch/'+output_path):\n",
    "    shutil.rmtree('/scratch/'+output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(output_path+'.tar')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import display_html\n",
    "display_html(site_chips_joined.showHTML(30, truncate=True), raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
