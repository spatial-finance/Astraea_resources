{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of RGB Sentinel-2 Chips for Model Build\n",
    "\n",
    "This notebook creates the Sentinel-2 RGB image chips for cement, steel, or landcover sites from the EarthAI catalog.\n",
    "\n",
    "* Site locations in China\n",
    "* Sentinel-2, red, green, and blue bands\n",
    "* Chips are 3-km on a side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as pys\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "* Set number of partitions on par with the number of catalog items per scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 250\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input and output files and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `site_type` should be set to `'cement'`, `'steel'`, or `'landcover'`\n",
    "* `year` defines the year of selected scenes\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `max_cc` is the maximum cloud coverage to use in eod query filter (in percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_type = 'landcover'\n",
    "year = '2020'\n",
    "\n",
    "chip_size = 300 # 3 km for Sentinel-2\n",
    "max_cc = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files\n",
    "\n",
    "* `site_geojson` is a GeoJSON specifying the chip centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_geojson = '../../resources/macro-loc-model-build4/'+site_type+'_chip_cntr_china_v4.1_s2.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `output_path` defines directory to write chip GeoTIFFs to, and sub-folder on S3 where chips are stored\n",
    "* `s3_path` defines S3 high-level folder for L8 TIR macro-localization data\n",
    "* `filename_append` is appended to each chip file name\n",
    "* `chip_extents_gjson` is an output GeoJSON file with chip metadata and tile extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'ALD_S2_RGB_'+site_type+'_chips_v4p1_'+year+'_train4'\n",
    "s3_path = 'S2-RGB-macro-localization-model-build4'\n",
    "\n",
    "filename_append = 'v4p1_'+year+'_S2_RGB'\n",
    "chip_extents_gjson = '../../resources/macro-loc-model-build4/'+output_path+'.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/scratch/'+output_path):\n",
    "    shutil.rmtree('/scratch/'+output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EOD Catalog Read and Chipping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get catalog of Sentinel-2 scenes that intersect with chip centroids\n",
    "\n",
    "Queries EarthAI Catalog to find S2 scenes that intersect with chip centroids. Returns scenes/datetimes from May - August in specified `year`, limited to scenes with less than `max_cc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eod_read_catalog(geom, year, max_cc=100):\n",
    "    \n",
    "    # Start/end date formatting\n",
    "    start_date = year+'-05-01'\n",
    "    end_date = year+'-08-31'\n",
    "    \n",
    "    # Query catalog\n",
    "    site_cat = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=start_date,\n",
    "        end_datetime=end_date,\n",
    "        max_cloud_cover=max_cc,\n",
    "        collections='sentinel2_l2a'\n",
    "        )\n",
    "    if len(site_cat) > 0:\n",
    "        return(site_cat)\n",
    "        \n",
    "    else:\n",
    "        return([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image Chips\n",
    "\n",
    "* Read and create image chips for specified chip centers\n",
    "* Select data from highest quality scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chips(site_cat, chip_size=300, site_type=site_type):\n",
    "    \n",
    "    # Uses centroid-centered chipping to create same-size chips\n",
    "    # Grabs red, green, and blue bands\n",
    "    # Filter out chips smaller than chip_size x chips_size\n",
    "    # Rename columns\n",
    "    # Filter out chips with NoData cells\n",
    "    # Normalize and convert data bands to uint16\n",
    "    if site_type=='landcover':\n",
    "        site_chip_all = spark.read.chip(site_cat, ['B04_10m','B03_10m','B02_10m'],\n",
    "                                    chipping_strategy=chp.CentroidCentered(chip_size, chip_size)) \\\n",
    "                         .select('tile_id', \n",
    "                                 'eod_grid_id', 'id', 'datetime', 'eo_cloud_cover', \n",
    "                                 'B04_10m', 'B03_10m', 'B02_10m') \\\n",
    "                         .withColumn('tile_dims', rf_dimensions('B04_10m')) \\\n",
    "                         .filter((pys.col('tile_dims').rows == chip_size) & \n",
    "                                 (pys.col('tile_dims').cols == chip_size)) \\\n",
    "                         .withColumnRenamed('eod_grid_id', 'scene_id') \\\n",
    "                         .withColumnRenamed('eo_cloud_cover', 'scene_cloud_pnt') \\\n",
    "                         .withColumn('Red_uint16', rf_convert_cell_type('B04_10m', 'uint16')) \\\n",
    "                         .withColumn('nodata_cell_cnt', rf_no_data_cells('Red_uint16')) \\\n",
    "                         .filter(pys.col('nodata_cell_cnt') == 0) \\\n",
    "                         .withColumn('Red', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B04_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .withColumn('Green', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B03_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .withColumn('Blue', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B02_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .drop('B04_10m', 'B03_10m', 'B02_10m', 'tile_dims', 'Red_uint16', 'nodata_cell_cnt') \\\n",
    "                         .cache()\n",
    "    else:\n",
    "        site_chip_all = spark.read.chip(site_cat, ['B04_10m','B03_10m','B02_10m'],\n",
    "                                    chipping_strategy=chp.CentroidCentered(chip_size, chip_size)) \\\n",
    "                         .select('uid', 'tile_id', 'dist_m', \n",
    "                                 'eod_grid_id', 'id', 'datetime', 'eo_cloud_cover', \n",
    "                                 'B04_10m', 'B03_10m', 'B02_10m') \\\n",
    "                         .withColumn('tile_dims', rf_dimensions('B04_10m')) \\\n",
    "                         .filter((pys.col('tile_dims').rows == chip_size) & \n",
    "                                 (pys.col('tile_dims').cols == chip_size)) \\\n",
    "                         .withColumnRenamed('eod_grid_id', 'scene_id') \\\n",
    "                         .withColumnRenamed('eo_cloud_cover', 'scene_cloud_pnt') \\\n",
    "                         .withColumn('Red_uint16', rf_convert_cell_type('B04_10m', 'uint16')) \\\n",
    "                         .withColumn('nodata_cell_cnt', rf_no_data_cells('Red_uint16')) \\\n",
    "                         .filter(pys.col('nodata_cell_cnt') == 0) \\\n",
    "                         .withColumn('Red', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B04_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .withColumn('Green', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B03_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .withColumn('Blue', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B02_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .drop('B04_10m', 'B03_10m', 'B02_10m', 'tile_dims', 'Red_uint16', 'nodata_cell_cnt') \\\n",
    "                         .cache()\n",
    "    \n",
    "    return(site_chip_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load site location point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_gdf = gpd.read_file(site_geojson)\n",
    "print(\"Total count of sites: \", len(site_gdf))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FOR TESTING ONLY - LIMIT NUMBER OF PLANTS\n",
    "site_gdf = site_gdf.head(12)\n",
    "site_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Catalog Covering All Chips\n",
    "\n",
    "* All Chips in specified year, from May to August, with less than specified cloud coverage\n",
    "* Determine unique scene ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cat_all = eod_read_catalog(site_gdf, year, max_cc=max_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_ids = site_cat_all.eod_grid_id.unique().tolist()\n",
    "scene_ids.sort()\n",
    "print(\"Total Number of Unique Scene Ids: \", len(scene_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chips\n",
    "\n",
    "* Loops over scene id's to speed up process\n",
    "* Finds best scene to create unique chip per scene\n",
    "* Writes chips to GeoTIFFs\n",
    "* Creates GeoJSON file with chip extents and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over scene ids\n",
    "for scene_id in scene_ids:\n",
    "    \n",
    "    # Limit catalog to scenes matching scene id\n",
    "    # Join to chip sites\n",
    "    site_cat = site_cat_all[site_cat_all.eod_grid_id == scene_id]\n",
    "    site_cat = gpd.sjoin(site_gdf, site_cat)\n",
    "    \n",
    "    # Create chips for all scenes\n",
    "    site_chips = create_chips(site_cat, chip_size=chip_size)\n",
    "    chp_cnt = site_chips.count()\n",
    "    \n",
    "    if (chp_cnt > 0):\n",
    "        \n",
    "        # For each tile_id, find the scene with the least cloud coverage\n",
    "        chpinf_pdf = site_chips.select('tile_id', 'id', 'scene_cloud_pnt').toPandas()\n",
    "        site_mincc_pdf = chpinf_pdf.sort_values('scene_cloud_pnt') \\\n",
    "                                   .groupby(['tile_id']).first() \\\n",
    "                                   .drop('scene_cloud_pnt', axis=1) \\\n",
    "                                   .reset_index()\n",
    "        \n",
    "        # Join to RasterFrame to find unique chip per tile_id\n",
    "        site_mincc_sdf = spark.createDataFrame(site_mincc_pdf) \\\n",
    "                              .withColumnRenamed('tile_id', 'tile_id2') \\\n",
    "                              .withColumnRenamed('id', 'id2')\n",
    "        site_chips_unq = site_chips.join(site_mincc_sdf, \n",
    "                                         (site_chips.tile_id == site_mincc_sdf.tile_id2) & \\\n",
    "                                         (site_chips.id == site_mincc_sdf.id2)) \\\n",
    "                                   .drop('tile_id2', 'id2') \\\n",
    "                                   .withColumn('file_path_name', \n",
    "                                               pys.concat_ws('_', pys.col('scene_id'), pys.col('tile_id'), \n",
    "                                                             lit(site_type), lit(filename_append))) \\\n",
    "                                   .cache()\n",
    "        \n",
    "        # Write chips to GeoTIFFs\n",
    "        site_chips_unq.write.chip('/scratch/'+output_path, filenameCol='file_path_name', \n",
    "                                  catalog=False)\n",
    "        \n",
    "        # Write out Vector File of Tile Extents and Metadata\n",
    "        site_chips_pdf = site_chips_unq.withColumn('tile_extent',\n",
    "                                                   st_reproject(st_geometry(rf_extent('Red')),\n",
    "                                                                rf_crs('Red'), lit('EPSG:4326'))) \\\n",
    "                                       .drop('Red', 'Green', 'Blue') \\\n",
    "                                       .toPandas()\n",
    "        site_chips_gdf = gpd.GeoDataFrame(site_chips_pdf.drop('tile_extent', axis=1),\n",
    "                                          geometry=site_chips_pdf.tile_extent,\n",
    "                                          crs='EPSG:4326')\n",
    "        \n",
    "        # Append to growing GeoDataFrame\n",
    "        if 'site_chip_ext_gdf' in locals():\n",
    "            site_chip_ext_gdf = pd.concat([site_chip_ext_gdf, site_chips_gdf], \n",
    "                                          ignore_index=True)\n",
    "        else:\n",
    "            site_chip_ext_gdf = site_chips_gdf\n",
    "            \n",
    "    print('Done creating chips for scene ', scene_id, '(', \\\n",
    "          scene_ids.index(scene_id)+1, ' out of ', len(scene_ids), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out tile extents to GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_chip_ext_gdf.to_file(chip_extents_gjson, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tar GeoTIFFs and Upload to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unix_code = 'tar -C /scratch -cvf '+output_path+'.tar '+output_path\n",
    "#os.system(unix_code)\n",
    "!tar -C /scratch -cvf {output_path+'.tar '} {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')\n",
    "\n",
    "bucket.upload_file(output_path+'.tar', \n",
    "                   s3_path+'/'+output_path+'.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/scratch/'+output_path):\n",
    "    shutil.rmtree('/scratch/'+output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(output_path+'.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
