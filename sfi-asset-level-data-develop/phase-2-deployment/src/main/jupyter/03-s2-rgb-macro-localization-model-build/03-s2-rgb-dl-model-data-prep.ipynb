{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Sentinel-2 RGB chips for Macro-localization Deep Learning Model Build\n",
    "\n",
    "This notebook prepares image chips to train a classifier on Sentinel-2 RGB image chips, and store them on AWS/S3. This version processes the data to fit a multiclass model on cement, steel, and landcover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download .tar Files From S3 Bucket and Extract Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "AWS_SOURCE_PATH = 'S2-RGB-macro-localization-model-build4'\n",
    "\n",
    "TARGET_PATH = '/scratch/ALD_S2_RGB_chips_v4p1_train4'\n",
    "\n",
    "IMG_DIRS = (\n",
    "    ('ALD_S2_RGB_landcover_chips_v4p1_2020_train4', 'landcover'),\n",
    "    ('ALD_S2_RGB_cement_chips_v4p1_2020_train4', 'cement'),\n",
    "    ('ALD_S2_RGB_steel_chips_v4p1_2020_train4', 'steel')\n",
    ")\n",
    "\n",
    "!mkdir -p {TARGET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')\n",
    "\n",
    "for source_file, _ in IMG_DIRS:\n",
    "    bucket.download_file(str(Path(AWS_SOURCE_PATH, source_file+'.tar')), \n",
    "                         str(Path(TARGET_PATH, source_file + '.tar')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_file, _ in IMG_DIRS:\n",
    "    !cd {TARGET_PATH} && tar xf {str(Path(source_file + '.tar'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the Data Using Stratified Random Sampling\n",
    "\n",
    "To help address the issue of limited sample sizes (in particular for steel plant imagery), we partitition the data using stratified random sampling.\n",
    "\n",
    "* Will define PNGs to put in train/ and validate/ folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = ! find {TARGET_PATH} | grep tif$\n",
    "class_assignments = [f.split('_')[-5] for f in image_list]\n",
    "\n",
    "train_idx, val_idx = next(sklearn.model_selection.StratifiedShuffleSplit(n_splits=2, random_state=42, test_size=0.2).split(class_assignments, class_assignments))\n",
    "subset_assignments = ['train' if i in train_idx else 'validate' for i in range(len(image_list))]\n",
    "\n",
    "output_pngs = [f.split('/')[-1].replace('tif', 'png') for f in image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_class in np.unique(class_assignments):\n",
    "    for subset in np.unique(subset_assignments):\n",
    "        !mkdir -p {TARGET_PATH}/{subset}/{image_class}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GeoTiff to PNG\n",
    "\n",
    "Fastai appears to require converting TIFF files to an alternative image format. Thus, convert from GeoTIFF to PNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(tif_filename, png_filename):\n",
    "    with rasterio.open(tif_filename) as infile:\n",
    "        \n",
    "        profile = infile.profile\n",
    "        profile['driver'] = 'PNG'\n",
    "        \n",
    "        raster = infile.read()\n",
    "        \n",
    "        with rasterio.open(png_filename, 'w', **profile) as dst:\n",
    "            dst.write(raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each image only if its corresponding target file does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_file, class_assignment, subset_assignment, png_file in zip(image_list, \n",
    "                                                                     class_assignments, \n",
    "                                                                     subset_assignments,\n",
    "                                                                     output_pngs):\n",
    "    if not Path(TARGET_PATH, subset_assignment, class_assignment, png_file).exists():\n",
    "        convert_image(image_file, \n",
    "                      Path(TARGET_PATH, subset_assignment, class_assignment, png_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out record of training/testing chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_pdf = pd.DataFrame({\"file\": output_pngs,\n",
    "                                 \"class\": class_assignments,\n",
    "                                 \"subset\": subset_assignments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_pdf.to_csv(\"../../resources/macro-loc-model-build4/\"+TARGET_PATH.split('/')[-1]+\"_record.csv\",\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tar Files and Upload to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_file, _ in IMG_DIRS:\n",
    "    shutil.rmtree(TARGET_PATH+'/'+source_file)\n",
    "    os.remove(TARGET_PATH+'/'+source_file+'.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -C /scratch -cvf '+TARGET_PATH.split('/')[-1]+'.tar '+TARGET_PATH.split('/')[-1]\n",
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.upload_file(TARGET_PATH.split('/')[-1]+'.tar', \n",
    "                   AWS_SOURCE_PATH+'/'+TARGET_PATH.split('/')[-1]+'.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TARGET_PATH)\n",
    "os.remove(TARGET_PATH.split('/')[-1]+'.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
