{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track progress of TIR Landsat 8 Macrolocalization Model\n",
    "\n",
    "This notebook tallies up the progress on scoring the TIR Landsat 8 macrolocalization models for cement and steel plants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input/output files and paths, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `year` defines the year used for model deployment\n",
    "* `scene_subset` set to 1 or 2. Scoring was divided in two pieces to run on two servers at the same time. 1 will process the first set of scenes; 2 will process the second. \n",
    "* `init_acct_file` set to True or False. If 1, will initialize the scene accounting file by computing the total chip count per scene. Only need to do this once per subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2018'\n",
    "scene_subset = 2\n",
    "init_acct_file = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files and paths\n",
    "\n",
    "* `s3_path` defines S3 high-level folder for L8 TIR macro-localization data\n",
    "* `chip_cntr_tar` is the tar with GeoJSON files of chip centroids for the deployment region\n",
    "* `score_tar` define tar of score GeoJSONS (one for each scene)\n",
    "* `LOCAL_DIR` specifies where to keep put files locally for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 'L8-TIR-macro-localization-model-deployment'\n",
    "chip_cntr_tar = 'L8-deployment-chip-centroids-CHN-10km-pthsh0.002.tar'\n",
    "score_tar = 'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+year+'_set'+str(scene_subset)+'.tar'\n",
    "\n",
    "LOCAL_DIR = '/scratch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `scene_acct_csv` defines csv file tallying number of chips scored for different years per scene. This is first created when `year = '2020'`, and updated for other years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_acct_csv = '../../resources/macro-loc-model-deployment/L8-deployment-scene_acct-CHN-10km-pthsh0.002_set'+str(scene_subset)+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Read in Chip Centroids from 10km Grid\n",
    "\n",
    "* Only necessary if `init_acct_file = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_acct_file:\n",
    "\n",
    "    # Download and unpack tar file\n",
    "    bucket.download_file(s3_path+'/'+chip_cntr_tar, LOCAL_DIR+chip_cntr_tar)\n",
    "    !tar -xf {LOCAL_DIR+chip_cntr_tar} -C {LOCAL_DIR}\n",
    "    \n",
    "    chip_cntr_dir = chip_cntr_tar.replace('.tar', '')\n",
    "    chip_cntr_gjsons = os.listdir(LOCAL_DIR+chip_cntr_dir)\n",
    "    chip_cntr_gjsons.sort()\n",
    "    \n",
    "    # Divide data by set\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for f in chip_cntr_gjsons:\n",
    "        scene_ind3 = int(f.split('.')[1].split('-')[-1][0:3])\n",
    "        if scene_ind3 <= 125:\n",
    "            list1.append(f)\n",
    "        else:\n",
    "            list2.append(f)\n",
    "            \n",
    "    if scene_subset == 1:\n",
    "        scene_files = [LOCAL_DIR+chip_cntr_dir+'/'+f for f in list1]\n",
    "        scene_ids = [f.split('_')[-1].split('.')[0] for f in list1]\n",
    "    if scene_subset == 2:\n",
    "        scene_files = [LOCAL_DIR+chip_cntr_dir+'/'+f for f in list2]\n",
    "        scene_ids = [f.split('_')[-1].split('.')[0] for f in list2]\n",
    "        \n",
    "    print('Total number of scenes: ', len(scene_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over Scenes, Tally Total Chip Count\n",
    "\n",
    "* Only if `init_acct_file = True`\n",
    "* Writes total count of chips per scene out to file\n",
    "* If `init_acct_file = False`, reads in the scene accounting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If init_acct_file = True, create new accounting file and save\n",
    "if init_acct_file:\n",
    "\n",
    "    # Define output DataFrame\n",
    "    scene_acct_pdf = pd.DataFrame(columns = ['scene_id', 'tile_cnt_tot'])\n",
    "    \n",
    "    # Loop over scenes\n",
    "    for scene_id, scene_file in zip(scene_ids, scene_files):\n",
    "    \n",
    "        # Read in chip centroids\n",
    "        chip_cntr_gdf = gpd.read_file(scene_file)\n",
    "        chip_cnt = len(chip_cntr_gdf)\n",
    "        \n",
    "        # Write chip count to DataFrame\n",
    "        scene_acct_pdf = scene_acct_pdf.append({'scene_id': scene_id,\n",
    "                                                'tile_cnt_tot': chip_cnt},\n",
    "                                               ignore_index=True)\n",
    "        \n",
    "        print('Scene ', scene_id, ': Total chip count = ', chip_cnt)\n",
    "        \n",
    "    # Save results to csv\n",
    "    scene_acct_pdf.to_csv(scene_acct_csv, index=False)\n",
    "    \n",
    "# Otherwise, load in existing file\n",
    "else:\n",
    "    scene_acct_pdf = pd.read_csv(scene_acct_csv, index_col=False)\n",
    "    print('Total number of scenes: ', len(scene_acct_pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Read in Scores for Tiles in Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.download_file(s3_path+'/'+score_tar, LOCAL_DIR+score_tar)\n",
    "!tar -xf {LOCAL_DIR+score_tar} -C {LOCAL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dir = score_tar.replace('.tar', '')\n",
    "score_gjsons = glob.glob(LOCAL_DIR+score_dir+'/*.geojson')\n",
    "score_gjsons.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in score_gjsons:\n",
    "    scene_ids = [f.split('_')[-1].split('.')[0] for f in score_gjsons]\n",
    "print('Total number of scored scenes: ', len(scene_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_acct_pdf = pd.DataFrame(columns = ['scene_id', 'tile_cnt_'+year])\n",
    "\n",
    "# Loop over scenes\n",
    "for scene_id, scene_file in zip(scene_ids, score_gjsons):\n",
    "    \n",
    "    # Read in scores\n",
    "    score_gdf = gpd.read_file(scene_file)\n",
    "    tile_cnt = len(score_gdf)\n",
    "        \n",
    "    # Write chip count to DataFrame\n",
    "    tile_acct_pdf = tile_acct_pdf.append({'scene_id': scene_id,\n",
    "                                          'tile_cnt_'+year: tile_cnt},\n",
    "                                         ignore_index=True)\n",
    "        \n",
    "    print('Scene ', scene_id, ': Total scored chip count = ', tile_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge scored chip count to total chip count DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_acct_pdf = pd.merge(scene_acct_pdf, tile_acct_pdf, how='left', on='scene_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_acct_pdf.loc[scene_acct_pdf['tile_cnt_'+year].isna(), ['tile_cnt_'+year]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_acct_pdf.to_csv(scene_acct_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
