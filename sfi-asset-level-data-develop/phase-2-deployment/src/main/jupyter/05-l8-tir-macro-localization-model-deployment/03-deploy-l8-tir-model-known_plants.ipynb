{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment of TIR Landsat 8 Macrolocalization Model - 2020\n",
    "\n",
    "This notebook deploys the TIR Landsat 8 macrolocalization models for cement and steel plants for the year 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==1.0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as pys\n",
    "from pyspark.sql.functions import lit, col, udf\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import glob\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "* Important to do this before defining the udfs for scoring\n",
    "* Set number of partitions on par with the number of catalog items per scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 2500\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input/output files and paths, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `unmsk_frac` is the minimum threshold on the fraction of unmasked cells required to keep site in sample\n",
    "* `year` defines the year for layer 1 (thermal band, in January); layers 2 and 3 (thermal band, in January and April, respectively) are `year - 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_size = 35 # 1.05 km for Landsat 8\n",
    "unmsk_frac = 0.75\n",
    "\n",
    "year = '2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files and paths\n",
    "\n",
    "* `s3_path` defines S3 high-level folder for L8 TIR macro-localization data\n",
    "* `cement_site_geojson` is GeoJSON of cement plants with exact locations\n",
    "* `steel_site_geojson` is GeoJSON of steel plants with exact locations\n",
    "* `CEMENT_MODEL_PATH` is the path on S3 to the Densenet161 cement model\n",
    "* `STEEL_MODEL_PATH` is the path on S3 to the Resnet50 steel model\n",
    "* `LOCAL_DIR` specifies where to keep put files locally for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 'L8-TIR-macro-localization-model-deployment'\n",
    "\n",
    "cement_site_geojson = \"../../resources/macro-loc-model-build/cement_exact_china_v4.1.geojson\"\n",
    "steel_site_geojson = \"../../resources/macro-loc-model-build/steel_exact_china_v4.1.geojson\"\n",
    "\n",
    "CEMENT_MODEL_PATH = 'L8-TIR-macro-localization-model-build3/L8-TIR-model-results3/densenet161_cement_binary_final.pkl'\n",
    "STEEL_MODEL_PATH = 'L8-TIR-macro-localization-model-build3/L8-TIR-model-results3/resnet50_steel_binary_final.pkl'\n",
    "\n",
    "LOCAL_DIR = '/scratch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `output_score_file` define output GeoJSON of scores for known plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_score_file = '../../resources/macro-loc-model-deployment/L8-known-plant-chip-fastai-scores-CHN-10km-pthsh0.002_'+year+'.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models and Define Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download models and load learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(MODEL_PATH):\n",
    "    if not os.path.exists(LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\")):\n",
    "        os.makedirs(LOCAL_DIR + MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "    bucket.download_file(MODEL_PATH, LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\") + \"/export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model(CEMENT_MODEL_PATH)\n",
    "download_model(STEEL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cement_model = load_learner(LOCAL_DIR + CEMENT_MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "steel_model = load_learner(LOCAL_DIR + STEEL_MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring function for PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pngs(path):\n",
    "    \n",
    "    # Get ImageDataBunch for Fastai\n",
    "    data = (ImageDataBunch.from_folder(path, train='all', bs=16, num_workers=0, seed=42).normalize(imagenet_stats))\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    data_cnt = len(data.train_ds)\n",
    "    uid = []\n",
    "    site_type = []\n",
    "    cement_prob = []\n",
    "    steel_prob = []\n",
    "    \n",
    "    # Loop over images and get scores and metadata\n",
    "    for i in range(0, data_cnt):\n",
    "        \n",
    "        # Cement results\n",
    "        p_cement = cement_model.predict(data.train_ds.x[i])\n",
    "        cement_prob.append(to_np(p_cement[2])[0].item())\n",
    "    \n",
    "        # Steel results\n",
    "        p_steel = steel_model.predict(data.train_ds.x[i])\n",
    "        steel_prob.append(to_np(p_steel[2])[1].item())\n",
    "    \n",
    "        # Metadata for chip\n",
    "        uid.append(str(data.items[i]).split('/')[-1].split('_')[0])\n",
    "        site_type.append(str(data.items[i]).split('_')[-1].split('.')[0])\n",
    "        \n",
    "    # Return data frame\n",
    "    score_pdf = pd.DataFrame({'uid': uid,\n",
    "                              'site_type': site_type,\n",
    "                              'cement_prob': cement_prob,\n",
    "                              'steel_prob': steel_prob})\n",
    "    \n",
    "    return(score_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EOD Catalog Read and Chipping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get catalog of Landsat 8 scenes that intersect with chip centroids\n",
    "\n",
    "Queries EarthAI Catalog to find L8 scenes that intersect with chip centroids.\n",
    "\n",
    "* Returns specified scene for:\n",
    "* January Year 2\n",
    "* January Year 1\n",
    "* April Year 1\n",
    "* Join back to chip centroids for chipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eod_read_catalog(geom, year):\n",
    "    \n",
    "    year2 = year\n",
    "    year1 = str(int(year2) - 1)\n",
    "    \n",
    "    # January Year 2\n",
    "    site_cat_year2_01 = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=year2+'-01-01', \n",
    "        end_datetime=year2+'-01-31',\n",
    "        max_cloud_cover=100,\n",
    "        collections='landsat8_l1tp'\n",
    "    )\n",
    "    if len(site_cat_year2_01) > 0:\n",
    "        site_cat_year2_01 = gpd.sjoin(geom, site_cat_year2_01)\n",
    "    \n",
    "    # January Year 1\n",
    "    site_cat_year1_01 = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=year1+'-01-01', \n",
    "        end_datetime=year1+'-01-31',\n",
    "        max_cloud_cover=100,\n",
    "        collections='landsat8_l1tp'\n",
    "    )\n",
    "    if len(site_cat_year1_01) > 0:\n",
    "        site_cat_year1_01 = gpd.sjoin(geom, site_cat_year1_01)\n",
    "    \n",
    "    # April Year 1\n",
    "    site_cat_year1_04 = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=year1+'-04-01', \n",
    "        end_datetime=year1+'-04-30',\n",
    "        max_cloud_cover=100,\n",
    "        collections='landsat8_l1tp'\n",
    "    )\n",
    "    if len(site_cat_year1_04) > 0:\n",
    "        site_cat_year1_04 = gpd.sjoin(geom, site_cat_year1_04)\n",
    "        \n",
    "    return({'site_cat_year2_01': site_cat_year2_01,\n",
    "            'site_cat_year1_01': site_cat_year1_01,\n",
    "            'site_cat_year1_04': site_cat_year1_04})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image Chips\n",
    "\n",
    "* Read and create image chips for 10km grid\n",
    "* Select highest quality chips per site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chips(site_cat, chip_size=35, unmsk_frac=0.75, col_suffix='JY2', repartition_size=partitions):\n",
    "    \n",
    "    # Create uniform, same-sized chips covering the deployment region\n",
    "    # Filter out blank chips at edge of scenes\n",
    "    # Handle rare edge case where returned chip is less than specified size (when reach edge of a scene)\n",
    "    # Mask chips by QA band and compute count of unmasked cells\n",
    "    # Remove chips with less than a minimum fraction of unmasked cells\n",
    "    site_chips = spark.read.chip(site_cat, ['BQA'],\n",
    "                                 chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                      .select('uid', 'id', 'BQA') \\\n",
    "                      .withColumn('mask', rf_make_constant_tile(1, chip_size, chip_size, 'uint16')) \\\n",
    "                      .withColumn('tot_cell_count', rf_data_cells('BQA')) \\\n",
    "                      .filter(pys.col('tot_cell_count') == chip_size*chip_size) \\\n",
    "                      .withColumn('BQA_min', rf_tile_min('BQA')) \\\n",
    "                      .filter(pys.col('BQA_min') > 1.0) \\\n",
    "                      .withColumn('mask', # designated fill = yes\n",
    "                                  rf_mask_by_bit('mask', 'BQA', 0, 1)) \\\n",
    "                      .withColumn('mask', # cloud = yes\n",
    "                                  rf_mask_by_bit('mask', 'BQA', 4, 1)) \\\n",
    "                      .withColumn('mask', # cloud shadow conf is medium or high\n",
    "                                  rf_mask_by_bits('mask', 'BQA', 7, 2, [2, 3])) \\\n",
    "                      .withColumn('mask', # cirrus conf is medium or high\n",
    "                                  rf_mask_by_bits('mask', 'BQA', 11, 2, [2, 3])) \\\n",
    "                      .withColumn('unmsk_cell_count', rf_data_cells('mask')) \\\n",
    "                      .filter(pys.col('unmsk_cell_count') >= unmsk_frac*chip_size*chip_size) \\\n",
    "                      .repartition(repartition_size, 'uid', 'id')\n",
    "    \n",
    "    # Find the chip(s) with the highest number of unmasked cells\n",
    "    # If there's >1 chip (a tie) take the first record\n",
    "    chpinf_pdf = site_chips.select('uid', 'id', 'unmsk_cell_count').toPandas()\n",
    "    chpinf_pdf['grpid'] = chpinf_pdf['uid']    \n",
    "    site_maxcnt = chpinf_pdf.sort_values('unmsk_cell_count', ascending=False) \\\n",
    "                            .groupby(['grpid']).first() \\\n",
    "                            .drop('unmsk_cell_count', axis=1)\n",
    "    \n",
    "    # Read in thermal band for highest quality chip\n",
    "    site_cat = site_cat.merge(site_maxcnt, on=['uid', 'id'], how='inner')\n",
    "    site_chips_unq = spark.read.chip(site_cat, ['B10'],\n",
    "                                     chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                          .select('uid', 'site_type', 'id', 'datetime', 'B10') \\\n",
    "                          .withColumn('B10'+'_'+col_suffix,\n",
    "                                      rf_convert_cell_type(rf_local_multiply(rf_rescale(rf_convert_cell_type('B10', 'uint16')), \n",
    "                                                                             65535), 'uint16')) \\\n",
    "                          .drop('B10') \\\n",
    "                          .withColumnRenamed('id', 'id'+'_'+col_suffix) \\\n",
    "                          .withColumnRenamed('datetime', 'datetime'+'_'+col_suffix) \\\n",
    "                          .repartition(repartition_size, 'uid')\n",
    "    \n",
    "    return(site_chips_unq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GeoTIFFs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(tif_filename, png_filename):\n",
    "    with rasterio.open(tif_filename) as infile:\n",
    "        \n",
    "        profile = infile.profile\n",
    "        profile['driver'] = 'PNG'\n",
    "        \n",
    "        raster = infile.read()\n",
    "        \n",
    "        with rasterio.open(png_filename, 'w', **profile) as dst:\n",
    "            dst.write(raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PNGs from RasterFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_from_rf(rf):\n",
    "    \n",
    "    # Delete temporary output paths for geotiffs and pngs and all files if they exist\n",
    "    if os.path.exists('geotiffs'):\n",
    "        shutil.rmtree('geotiffs')\n",
    "    if os.path.exists('pngs'):\n",
    "        shutil.rmtree('pngs')\n",
    "    \n",
    "    # Create GeoTIFFs from RasterFrame\n",
    "    rf.write.chip('geotiffs', filenameCol='file_path_name', catalog=False)\n",
    "    tif_file_list = glob.glob('geotiffs/*.tif')\n",
    "    \n",
    "    # Create output paths for PNGs to fit Fastai structure\n",
    "    os.mkdir('pngs')\n",
    "    os.mkdir('pngs/all')\n",
    "    png_file_list = [f.replace('.tif', '.png').replace('geotiffs/', 'pngs/all/') for f in tif_file_list]\n",
    "    \n",
    "    # Convert and write out PNGs\n",
    "    for i in range(0, len(tif_file_list)):\n",
    "        convert_image(tif_file_list[i], png_file_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Output Function\n",
    "\n",
    "* Writes out scores to GeoJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chip_scores(rf, pdf, year):\n",
    "    \n",
    "    # Get tile extents from RasterFrame\n",
    "    geo_pdf = rf.withColumn('geometry', st_reproject(st_geometry(rf_extent('B10_JY2')), \n",
    "                                                     rf_crs('B10_JY2'), \n",
    "                                                     pys.lit('EPSG:4326'))) \\\n",
    "                .select('uid', 'site_type', 'geometry').toPandas()\n",
    "    geo_pdf['year'] = year\n",
    "    geo_gdf = gpd.GeoDataFrame(geo_pdf, geometry='geometry', crs='EPSG:4326')\n",
    "    \n",
    "    # Join with scores\n",
    "    scores_gdf = pd.merge(geo_gdf, pdf, how='inner', on=['uid', 'site_type'])\n",
    "    \n",
    "    scores_gdf.to_file(output_score_file, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Cement and Steel Plant sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cement_site_gdf = gpd.read_file(cement_site_geojson)\n",
    "cement_site_gdf['site_type'] = 'cement'\n",
    "steel_site_gdf = gpd.read_file(steel_site_geojson)\n",
    "steel_site_gdf['site_type'] = 'steel'\n",
    "chip_cntr_gdf = pd.concat([cement_site_gdf, steel_site_gdf], ignore_index=True)\n",
    "chip_cnt = len(chip_cntr_gdf)\n",
    "print(\"Total count of sites: \", chip_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of cement sites: \", len(chip_cntr_gdf[chip_cntr_gdf['site_type'] == 'cement']))\n",
    "print(\"Number of steel sites: \", len(chip_cntr_gdf[chip_cntr_gdf['site_type'] == 'steel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chips and Score\n",
    "\n",
    "For each site:\n",
    "\n",
    "* Get catalog of Landsat 8 scenes that intersect with chip centroids\n",
    "* Read and create image chips\n",
    "* Join TIR chips at different dates into single RasterFrame and score models\n",
    "* Write scores out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get catalog of Landsat 8 scenes that intersect with chip centroids\n",
    "site_cat_list = eod_read_catalog(chip_cntr_gdf, year)\n",
    "    \n",
    "# Read and create image chips\n",
    "# ---------------------------\n",
    "        \n",
    "# January Year 2\n",
    "site_chip_year2_01_unq = create_chips(site_cat_list['site_cat_year2_01'], \n",
    "                                      chip_size=chip_size, \n",
    "                                      unmsk_frac=unmsk_frac, \n",
    "                                      col_suffix='JY2',\n",
    "                                      repartition_size=round(chip_cnt/4))\n",
    "        \n",
    "# January Year 1\n",
    "site_chip_year1_01_unq = create_chips(site_cat_list['site_cat_year1_01'], \n",
    "                                      chip_size=chip_size, \n",
    "                                      unmsk_frac=unmsk_frac,\n",
    "                                      col_suffix='JY1',\n",
    "                                      repartition_size=round(chip_cnt/4))\n",
    "        \n",
    "# April Year 1\n",
    "site_chip_year1_04_unq = create_chips(site_cat_list['site_cat_year1_04'],\n",
    "                                      chip_size=chip_size,\n",
    "                                      unmsk_frac=unmsk_frac,\n",
    "                                      col_suffix='AY1',\n",
    "                                      repartition_size=round(chip_cnt/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join TIR chips\n",
    "site_chips_joined = site_chip_year2_01_unq.join(site_chip_year1_01_unq, on=['uid', 'site_type'], how='inner') \\\n",
    "                                          .join(site_chip_year1_04_unq, on=['uid', 'site_type'], how='inner') \\\n",
    "                                          .withColumn('file_path_name', \n",
    "                                                      pys.concat_ws('_', pys.col('uid'), pys.col('site_type'))) \\\n",
    "                                          .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out temporary PNGs to score\n",
    "png_from_rf(site_chips_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score PNGs\n",
    "deployment_scores_pdf = score_pngs('pngs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write scores to GeoJSON file\n",
    "write_chip_scores(site_chips_joined, deployment_scores_pdf, year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
