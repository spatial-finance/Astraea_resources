{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment of TIR Landsat 8 Macrolocalization Model - 2020\n",
    "\n",
    "This notebook deploys the TIR Landsat 8 macrolocalization models for cement and steel plants for the year 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==1.0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as pys\n",
    "from pyspark.sql.functions import lit, col, udf\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "* Important to do this before defining the udfs for scoring\n",
    "* Set number of partitions on par with the number of catalog items per scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 2500\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input/output files and paths, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `unmsk_frac` is the minimum threshold on the fraction of unmasked cells required to keep site in sample\n",
    "* `year` defines the year for layer 1 (thermal band, in January); layers 2 and 3 (thermal band, in January and April, respectively) are `year - 1`\n",
    "* `scene_subset` set to 1 or 2. This divides the scoring in two pieces to run on two servers at the same time. 1 will process the first set of scenes; 2 will process the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_size = 35 # 1.05 km for Landsat 8\n",
    "unmsk_frac = 0.75\n",
    "\n",
    "year = '2020'\n",
    "\n",
    "scene_subset = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files and paths\n",
    "\n",
    "* `s3_path` defines S3 high-level folder for L8 TIR macro-localization data\n",
    "* `chip_cntr_tar` is the tar with GeoJSON files of chip centroids for the deployment region\n",
    "* `CEMENT_MODEL_PATH` is the path on S3 to the Densenet161 cement model\n",
    "* `STEEL_MODEL_PATH` is the path on S3 to the Resnet50 steel model\n",
    "* `LOCAL_DIR` specifies where to keep put files locally for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 'L8-TIR-macro-localization-model-deployment'\n",
    "chip_cntr_tar = 'L8-deployment-chip-centroids-CHN-10km-pthsh0.002.tar'\n",
    "\n",
    "CEMENT_MODEL_PATH = 'L8-TIR-macro-localization-model-build3/L8-TIR-model-results3/densenet161_cement_binary_final.pkl'\n",
    "STEEL_MODEL_PATH = 'L8-TIR-macro-localization-model-build3/L8-TIR-model-results3/resnet50_steel_binary_final.pkl'\n",
    "\n",
    "LOCAL_DIR = '/scratch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `output_path` defines (temporary) local place of storage\n",
    "* `output_score_tar` define output tar of score GeoJSONS (one for each scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+year+'_set'+str(scene_subset)\n",
    "output_score_tar = output_path+'.tar'\n",
    "output_gjson_prefix = 'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+year+'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models and Define Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download models and load learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(MODEL_PATH):\n",
    "    if not os.path.exists(LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\")):\n",
    "        os.makedirs(LOCAL_DIR + MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "    bucket.download_file(MODEL_PATH, LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\") + \"/export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model(CEMENT_MODEL_PATH)\n",
    "download_model(STEEL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cement_model = load_learner(LOCAL_DIR + CEMENT_MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "steel_model = load_learner(LOCAL_DIR + STEEL_MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring function for PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pngs(path):\n",
    "    \n",
    "    # Get ImageDataBunch for Fastai\n",
    "    data = (ImageDataBunch.from_folder(path, train='all', bs=16, num_workers=0, seed=42).normalize(imagenet_stats))\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    data_cnt = len(data.train_ds)\n",
    "    scene_id = []\n",
    "    tile_id = []\n",
    "    cement_prob = []\n",
    "    steel_prob = []\n",
    "    \n",
    "    # Loop over images and get scores and metadata\n",
    "    for i in range(0, data_cnt):\n",
    "        \n",
    "        # Cement results\n",
    "        p_cement = cement_model.predict(data.train_ds.x[i])\n",
    "        cement_prob.append(to_np(p_cement[2])[0].item())\n",
    "    \n",
    "        # Steel results\n",
    "        p_steel = steel_model.predict(data.train_ds.x[i])\n",
    "        steel_prob.append(to_np(p_steel[2])[1].item())\n",
    "    \n",
    "        # Metadata for chip\n",
    "        scene_id.append('-'.join(str(data.items[i]).split('/')[-1].split('-')[0:2]))\n",
    "        tile_id.append(str(data.items[i]).split('/')[-1].split('.')[0])\n",
    "        \n",
    "    # Return data frame\n",
    "    score_pdf = pd.DataFrame({'scene_id': scene_id,\n",
    "                              'tile_id': tile_id,\n",
    "                              'cement_prob': cement_prob,\n",
    "                              'steel_prob': steel_prob})\n",
    "    \n",
    "    return(score_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EOD Catalog Read and Chipping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get catalog of Landsat 8 scenes that intersect with chip centroids\n",
    "\n",
    "Queries EarthAI Catalog to find L8 scenes that intersect with chip centroids.\n",
    "\n",
    "* Returns specified scene for:\n",
    "* January Year 2\n",
    "* January Year 1\n",
    "* April Year 1\n",
    "* Join back to chip centroids for chipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eod_read_catalog(geom, grid_id, year):\n",
    "    \n",
    "    year2 = year\n",
    "    year1 = str(int(year2) - 1)\n",
    "    \n",
    "    # January Year 2\n",
    "    site_cat_year2_01 = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=year2+'-01-01', \n",
    "        end_datetime=year2+'-01-31',\n",
    "        max_cloud_cover=100,\n",
    "        collections='landsat8_l1tp',\n",
    "        grid_ids=[grid_id]\n",
    "    )\n",
    "    if len(site_cat_year2_01) > 0:\n",
    "        site_cat_year2_01 = gpd.sjoin(geom, site_cat_year2_01)\n",
    "    \n",
    "    # January Year 1\n",
    "    site_cat_year1_01 = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=year1+'-01-01', \n",
    "        end_datetime=year1+'-01-31',\n",
    "        max_cloud_cover=100,\n",
    "        collections='landsat8_l1tp',\n",
    "        grid_ids=[grid_id]\n",
    "    )\n",
    "    if len(site_cat_year1_01) > 0:\n",
    "        site_cat_year1_01 = gpd.sjoin(geom, site_cat_year1_01)\n",
    "    \n",
    "    # April Year 1\n",
    "    site_cat_year1_04 = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=year1+'-04-01', \n",
    "        end_datetime=year1+'-04-30',\n",
    "        max_cloud_cover=100,\n",
    "        collections='landsat8_l1tp',\n",
    "        grid_ids=[grid_id]\n",
    "    )\n",
    "    if len(site_cat_year1_04) > 0:\n",
    "        site_cat_year1_04 = gpd.sjoin(geom, site_cat_year1_04)\n",
    "        \n",
    "    return({'site_cat_year2_01': site_cat_year2_01,\n",
    "            'site_cat_year1_01': site_cat_year1_01,\n",
    "            'site_cat_year1_04': site_cat_year1_04})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image Chips\n",
    "\n",
    "* Read and create image chips for 10km grid\n",
    "* Select highest quality chips per site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chips(site_cat, chip_size=35, unmsk_frac=0.75, col_suffix='JY2', repartition_size=partitions):\n",
    "    \n",
    "    # Create uniform, same-sized chips covering the deployment region\n",
    "    # Filter out blank chips at edge of scenes\n",
    "    # Handle rare edge case where returned chip is less than specified size (when reach edge of a scene)\n",
    "    # Mask chips by QA band and compute count of unmasked cells\n",
    "    # Remove chips with less than a minimum fraction of unmasked cells\n",
    "    site_chips = spark.read.chip(site_cat, ['BQA'],\n",
    "                                 chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                      .select('scene_id', 'tile_id', 'id', 'BQA') \\\n",
    "                      .withColumn('mask', rf_make_constant_tile(1, chip_size, chip_size, 'uint16')) \\\n",
    "                      .withColumn('tot_cell_count', rf_data_cells('BQA')) \\\n",
    "                      .filter(pys.col('tot_cell_count') == chip_size*chip_size) \\\n",
    "                      .withColumn('BQA_min', rf_tile_min('BQA')) \\\n",
    "                      .filter(pys.col('BQA_min') > 1.0) \\\n",
    "                      .withColumn('mask', # designated fill = yes\n",
    "                                  rf_mask_by_bit('mask', 'BQA', 0, 1)) \\\n",
    "                      .withColumn('mask', # cloud = yes\n",
    "                                  rf_mask_by_bit('mask', 'BQA', 4, 1)) \\\n",
    "                      .withColumn('mask', # cloud shadow conf is medium or high\n",
    "                                  rf_mask_by_bits('mask', 'BQA', 7, 2, [2, 3])) \\\n",
    "                      .withColumn('mask', # cirrus conf is medium or high\n",
    "                                  rf_mask_by_bits('mask', 'BQA', 11, 2, [2, 3])) \\\n",
    "                      .withColumn('unmsk_cell_count', rf_data_cells('mask')) \\\n",
    "                      .filter(pys.col('unmsk_cell_count') >= unmsk_frac*chip_size*chip_size) \\\n",
    "                      .repartition(repartition_size, 'tile_id', 'id')\n",
    "    \n",
    "    # Find the chip(s) with the highest number of unmasked cells\n",
    "    # If there's >1 chip (a tie) take the first record\n",
    "    chpinf_pdf = site_chips.select('tile_id', 'id', 'unmsk_cell_count').toPandas()\n",
    "    chpinf_pdf['grpid'] = chpinf_pdf['tile_id']    \n",
    "    site_maxcnt = chpinf_pdf.sort_values('unmsk_cell_count', ascending=False) \\\n",
    "                            .groupby(['grpid']).first() \\\n",
    "                            .drop('unmsk_cell_count', axis=1)\n",
    "    \n",
    "    # Read in thermal band for highest quality chip\n",
    "    site_cat = site_cat.merge(site_maxcnt, on=['tile_id', 'id'], how='inner')\n",
    "    \n",
    "    if len(site_cat) > 0:\n",
    "        site_chips_unq = spark.read.chip(site_cat, ['B10'],\n",
    "                                     chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                          .select('scene_id', 'tile_id', 'id', 'datetime', 'B10') \\\n",
    "                          .withColumn('B10'+'_'+col_suffix,\n",
    "                                      rf_convert_cell_type(rf_local_multiply(rf_rescale(rf_convert_cell_type('B10', 'uint16')), \n",
    "                                                                             65535), 'uint16')) \\\n",
    "                          .drop('B10') \\\n",
    "                          .withColumnRenamed('id', 'id'+'_'+col_suffix) \\\n",
    "                          .withColumnRenamed('datetime', 'datetime'+'_'+col_suffix) \\\n",
    "                          .repartition(repartition_size, 'tile_id')\n",
    "    else:\n",
    "        site_chips_unq = None\n",
    "    \n",
    "    return(site_chips_unq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GeoTIFFs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(tif_filename, png_filename):\n",
    "    with rasterio.open(tif_filename) as infile:\n",
    "        \n",
    "        profile = infile.profile\n",
    "        profile['driver'] = 'PNG'\n",
    "        \n",
    "        raster = infile.read()\n",
    "        \n",
    "        with rasterio.open(png_filename, 'w', **profile) as dst:\n",
    "            dst.write(raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PNGs from RasterFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_from_rf(rf):\n",
    "    \n",
    "    # Create GeoTIFFs from RasterFrame\n",
    "    print('(Writing GeoTIFFs)')\n",
    "    st = time.time()\n",
    "    rf.write.chip('geotiffs', filenameCol='tile_id', catalog=False)\n",
    "    print('(', time.time()-st, ')')\n",
    "    tif_file_list = glob.glob('geotiffs/*.tif')\n",
    "    \n",
    "    # Create output paths for PNGs to fit Fastai structure\n",
    "    os.mkdir('pngs')\n",
    "    os.mkdir('pngs/all')\n",
    "    png_file_list = [f.replace('.tif', '.png').replace('geotiffs/', 'pngs/all/') for f in tif_file_list]\n",
    "    \n",
    "    # Convert and write out PNGs\n",
    "    print('(Writing PNGs)')\n",
    "    st = time.time()\n",
    "    for i in range(0, len(tif_file_list)):\n",
    "        convert_image(tif_file_list[i], png_file_list[i])\n",
    "    print('(', time.time()-st, ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Output Function\n",
    "\n",
    "* Writes out scores to GeoJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chip_scores(rf, pdf, year):\n",
    "    \n",
    "    # Get tile extents from RasterFrame\n",
    "    geo_pdf = rf.withColumn('geometry', st_reproject(st_geometry(rf_extent('B10_JY2')), \n",
    "                                                     rf_crs('B10_JY2'), \n",
    "                                                     pys.lit('EPSG:4326'))) \\\n",
    "                .select('scene_id', 'tile_id', 'geometry').toPandas()\n",
    "    geo_pdf['year'] = year\n",
    "    geo_gdf = gpd.GeoDataFrame(geo_pdf, geometry='geometry', crs='EPSG:4326')\n",
    "    \n",
    "    # Join with scores\n",
    "    scores_gdf = pd.merge(geo_gdf, pdf, how='inner', on=['scene_id', 'tile_id'])\n",
    "    \n",
    "    output_score_file = output_path+'/'+output_gjson_prefix+scores_gdf.scene_id[0]+'.geojson'\n",
    "    scores_gdf.to_file(output_score_file, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Read in Chip Centroids from 10km Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.download_file(s3_path+'/'+chip_cntr_tar, LOCAL_DIR+chip_cntr_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf {LOCAL_DIR+chip_cntr_tar} -C {LOCAL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_cntr_dir = chip_cntr_tar.replace('.tar', '')\n",
    "chip_cntr_gjsons = os.listdir(LOCAL_DIR+chip_cntr_dir)\n",
    "chip_cntr_gjsons.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split scoring effort in two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "for f in chip_cntr_gjsons:\n",
    "    scene_ind3 = int(f.split('.')[1].split('-')[-1][0:3])\n",
    "    if scene_ind3 <= 125:\n",
    "        list1.append(f)\n",
    "    else:\n",
    "        list2.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scene_subset == 1:\n",
    "    scene_files = [LOCAL_DIR+chip_cntr_dir+'/'+f for f in list1]\n",
    "    scene_ids = [f.split('_')[-1].split('.')[0] for f in list1]\n",
    "if scene_subset == 2:\n",
    "    scene_files = [LOCAL_DIR+chip_cntr_dir+'/'+f for f in list2]\n",
    "    scene_ids = [f.split('_')[-1].split('.')[0] for f in list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scene_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fail-safe\n",
    "\n",
    "If server crashes, this picks up where we left off, so don't have to rerun scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_scene_list = os.listdir(output_path)\n",
    "scored_scene_list.sort()\n",
    "if len(scored_scene_list) > 0:\n",
    "    last_scored_scene = scored_scene_list[-1].split('.')[1].split('_')[-1]\n",
    "    last_ind = scene_ids.index(last_scored_scene)\n",
    "    scene_files = scene_files[last_ind+1:]\n",
    "    scene_ids = scene_ids[last_ind+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scene_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary code to score specific scenes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scene_ids = ['WRS2-121033', 'WRS2-122032', 'WRS2-122033']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scene_files = ['/scratch/L8-deployment-chip-centroids-CHN-10km-pthsh0.002/L8-deployment-chip-centroids-CHN-10km-pthsh0.002_WRS2-121033.geojson',\n",
    "               '/scratch/L8-deployment-chip-centroids-CHN-10km-pthsh0.002/L8-deployment-chip-centroids-CHN-10km-pthsh0.002_WRS2-122032.geojson',\n",
    "               '/scratch/L8-deployment-chip-centroids-CHN-10km-pthsh0.002/L8-deployment-chip-centroids-CHN-10km-pthsh0.002_WRS2-122033.geojson']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over Scenes, Create Chips, and Score\n",
    "\n",
    "For each scene:\n",
    "\n",
    "* Get catalog of Landsat 8 scenes that intersect with chip centroids\n",
    "* Read and create image chips\n",
    "* Join TIR chips at different dates into single RasterFrame and score models\n",
    "* Write scores out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary output paths for geotiffs and pngs and all files if they exist\n",
    "if os.path.exists('geotiffs'):\n",
    "    print('(Deleting geotiffs)')\n",
    "    shutil.rmtree('geotiffs')\n",
    "if os.path.exists('pngs'):\n",
    "    print('(Deleting pngs)')\n",
    "    shutil.rmtree('pngs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start loop over scenes\n",
    "for scene_id, scene_file in zip(scene_ids, scene_files):\n",
    "    \n",
    "    # Track time\n",
    "    si_all = time.time()\n",
    "    \n",
    "    # Read in chips to GeoDataFrame\n",
    "    chip_cntr_gdf = gpd.read_file(scene_file)\n",
    "    chip_cnt = len(chip_cntr_gdf)\n",
    "    print('Scene ', scene_id, ': Total chip count = ', chip_cnt)\n",
    "    \n",
    "    # Get catalog of Landsat 8 scenes that intersect with chip centroids\n",
    "    site_cat_list = eod_read_catalog(chip_cntr_gdf, scene_id, year)\n",
    "    \n",
    "    # If one or more scenes missing for specified dates, do not score\n",
    "    if len(site_cat_list['site_cat_year2_01']) == 0 or \\\n",
    "       len(site_cat_list['site_cat_year1_01']) == 0 or \\\n",
    "       len(site_cat_list['site_cat_year1_04']) == 0:\n",
    "        \n",
    "        print('Scene ', scene_id, ': Cannot score for year ', year, ' (one or more scenes unavailable)')\n",
    "        print('Scene ', scene_id, ': Total time = ', (time.time() - si_all)/60., ' min')\n",
    "    \n",
    "    # If all dates available, chip and score\n",
    "    else:\n",
    "        \n",
    "        # Read and create image chips\n",
    "        # ---------------------------\n",
    "        \n",
    "        # January Year 2\n",
    "        print('Scene ', scene_id, ': Creating chips for Janary ', year)\n",
    "        st = time.time()\n",
    "        site_chip_year2_01_unq = create_chips(site_cat_list['site_cat_year2_01'], \n",
    "                                              chip_size=chip_size, \n",
    "                                              unmsk_frac=unmsk_frac, \n",
    "                                              col_suffix='JY2',\n",
    "                                              repartition_size=round(chip_cnt/4))\n",
    "        print('(', time.time() - st, ')')\n",
    "        \n",
    "        # January Year 1\n",
    "        st = time.time()\n",
    "        print('Scene ', scene_id, ': Creating chips for Janary ', str(int(year)-1))\n",
    "        site_chip_year1_01_unq = create_chips(site_cat_list['site_cat_year1_01'], \n",
    "                                              chip_size=chip_size, \n",
    "                                              unmsk_frac=unmsk_frac, \n",
    "                                              col_suffix='JY1',\n",
    "                                              repartition_size=round(chip_cnt/4))\n",
    "        print('(', time.time() - st, ')')\n",
    "        \n",
    "        # April Year 1\n",
    "        st = time.time()\n",
    "        print('Scene ', scene_id, ': Creating chips for April ', str(int(year)-1))\n",
    "        site_chip_year1_04_unq = create_chips(site_cat_list['site_cat_year1_04'], \n",
    "                                              chip_size=chip_size, \n",
    "                                              unmsk_frac=unmsk_frac, \n",
    "                                              col_suffix='AY1',\n",
    "                                              repartition_size=round(chip_cnt/4))\n",
    "        print('(', time.time() - st, ')')\n",
    "        \n",
    "        # Join TIR chips and score\n",
    "        if (site_chip_year2_01_unq is not None) and (site_chip_year1_01_unq is not None) and \\\n",
    "           (site_chip_year1_04_unq is not None):\n",
    "            \n",
    "            # Join TIR chips\n",
    "            st = time.time()\n",
    "            site_chips_joined = site_chip_year2_01_unq.join(site_chip_year1_01_unq, \n",
    "                                                            on=['scene_id', 'tile_id'], how='inner') \\\n",
    "                                                      .join(site_chip_year1_04_unq, \n",
    "                                                            on=['scene_id', 'tile_id'], how='inner') \\\n",
    "                                                      .repartition(round(chip_cnt/4), 'tile_id') \\\n",
    "                                                      .cache()\n",
    "            \n",
    "            if (site_chips_joined.count() > 0):\n",
    "            \n",
    "                # Write out temporary PNGs to score\n",
    "                print('Scene ', scene_id, ': Writing out GeoTIFFs and PNGs to score')\n",
    "                png_from_rf(site_chips_joined)\n",
    "                print('(', time.time() - st, ')')\n",
    "            \n",
    "                # Score PNGs\n",
    "                st = time.time()\n",
    "                print('Scene ', scene_id, ': Scoring chips')\n",
    "                deployment_scores_pdf = score_pngs('pngs')\n",
    "                print('(', time.time() - st, ')')\n",
    "            \n",
    "                # Delete temporary output paths for geotiffs and pngs and all files if they exist\n",
    "                if os.path.exists('geotiffs'):\n",
    "                    print('(Deleting geotiffs)')\n",
    "                    st = time.time()\n",
    "                    shutil.rmtree('geotiffs')\n",
    "                    print('(', time.time()-st, ')')\n",
    "                if os.path.exists('pngs'):\n",
    "                    print('(Deleting pngs)')\n",
    "                    st = time.time()\n",
    "                    shutil.rmtree('pngs')\n",
    "                    print('(', time.time()-st, ')')\n",
    "        \n",
    "                # Write scores to GeoJSON file\n",
    "                st = time.time()\n",
    "                print('Scene ', scene_id, ': Saving scores in GeoJSON')\n",
    "                write_chip_scores(site_chips_joined, deployment_scores_pdf, year)\n",
    "                print('(', time.time() - st, ')')\n",
    "            \n",
    "                print('Scene ', scene_id, ': Wrote ', len(deployment_scores_pdf), ' chips for year ', year)\n",
    "                print('Scene ', scene_id, ': Total time = ', (time.time() - si_all)/60., ' min')\n",
    "                \n",
    "            else:\n",
    "                print('Scene ', scene_id, ': Cannot score for year ', year, ' (no cloud-free chips available)')\n",
    "                print('Scene ', scene_id, ': Total time = ', (time.time() - si_all)/60., ' min')\n",
    "            \n",
    "        else:\n",
    "            print('Scene ', scene_id, ': Cannot score for year ', year, ' (no cloud-free chips available)')\n",
    "            print('Scene ', scene_id, ': Total time = ', (time.time() - si_all)/60., ' min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tar results and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -cvf '+output_score_tar+' '+output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.upload_file(output_score_tar, \n",
    "                   s3_path+'/'+output_score_tar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
