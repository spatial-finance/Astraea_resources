{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defines Landsat 8 Deployment Chip Centers for 10km Grid\n",
    "\n",
    "This notebook creates a GeoJSON file defining chip centers at which to deploy the Landsat 8 TIR macro-localization model.\n",
    "\n",
    "This addresses the issue that Landsat 8 scenes with the same grid id taken at different dates do not map to the exact same projected extents, which is required when combining these images in the 3-band dataset for deployment. This code thus defines a per-scene grid of tile centroids that we can use to create chips of the desired size, centered at the same lat/long.\n",
    "\n",
    "* Uses deployment regions defined from 10km Grid output from previous step\n",
    "* Uses Landsat scenes from catalog defined in previous step to define a scene-aligned grid\n",
    "* Outputs centers of tiles as deployment chip centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as F\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import shutil\n",
    "import glob\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input and output files and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `pred_thresh` is the prediction threshold for selecting deployment grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_size = 35 # 1.05 km for Landsat 8\n",
    "pred_thresh = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files\n",
    "\n",
    "* `deployment_gjson` is output GeoJSON of the deployment region\n",
    "* `catalog_csv` is a csv file of the catalog returned from EOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_gjson = '../../resources/macro-loc-model-deployment/L8-deployment-region-CHN-10km-pthsh'+str(pred_thresh)+'.geojson'\n",
    "catalog_csv = '../../resources/macro-loc-model-deployment/L8-deployment-catalog-CHN-10km-pthsh'+str(pred_thresh)+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `output_path` defines directory to write data to\n",
    "* `chip_extents_gjson_prefix` is output prefix for GeoJSON files of chip extents\n",
    "* `chip_centroids_gjson_prefix` is an output prefix for GeoJSON files with centroids of chip extents\n",
    "* `s3_path` is tag for S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../resources/macro-loc-model-deployment/l8-chip-centers/'\n",
    "chip_extents_gjson_prefix = 'L8-deployment-chip-extents-CHN-10km-pthsh'+str(pred_thresh)+'_'\n",
    "chip_centroids_gjson_prefix = 'L8-deployment-chip-centroids-CHN-10km-pthsh'+str(pred_thresh)+'_'\n",
    "\n",
    "s3_path = 'L8-TIR-macro-localization-model-deployment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in and join deployment region to Landsat 8 catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in deployment region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_deployment_gdf = gpd.read_file(deployment_gjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Landsat 8 catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cat_pdf = pd.read_csv(catalog_csv, index_col=False)\n",
    "site_cat_gdf = gpd.GeoDataFrame(site_cat_pdf,\n",
    "                                geometry=site_cat_pdf.geometry.apply(wkt.loads),\n",
    "                                crs='EPSG:4326')\n",
    "site_cat_gdf.eod_epsg4326_geometry_simplified = site_cat_gdf.eod_epsg4326_geometry_simplified.apply(wkt.loads)\n",
    "site_cat_gdf.proj_geometry = site_cat_gdf.proj_geometry.apply(wkt.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cat_gdf = gpd.sjoin(macro_deployment_gdf, site_cat_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cnt = site_cat_gdf['index'].nunique()\n",
    "l8_scene_cnt = site_cat_gdf.eod_grid_id.nunique()\n",
    "cat_cnt = len(site_cat_gdf)\n",
    "print('Number of Geometries in deployment region: ', reg_cnt)\n",
    "print('Number of Landsat 8 scenes in deployment regions: ', l8_scene_cnt)\n",
    "print('Number of catalog entries: ', cat_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark\n",
    "\n",
    "Set the number of partitions to be proportional to catalog size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = round(len(site_cat_gdf) / 4)\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and create image chips for 10km grid\n",
    "\n",
    "* Uses chip reader to create uniform, same-sized chips covering the deployment region w/ SceneAlignedGrid\n",
    "* Filter out blank chips at edge of scenes\n",
    "* Handle rare edge case where returned chip is less than specified size (when reach edge of a scene)\n",
    "* Compute tile extents in EPSG:4326\n",
    "* Find distinct Landsat-8 grid + tile rows\n",
    "\n",
    "Loops over number of scenes and creates separate GeoJSON files for each scene. This helps avoid growing large vector files and is more resilient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "l8_unq_scenes = site_cat_gdf.sort_values('eod_grid_id').eod_grid_id.unique()\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, l8_scene_cnt):\n",
    "    \n",
    "    # Check for existence of output file\n",
    "    scene_id = l8_unq_scenes[i]\n",
    "    output_gjson = output_path+chip_centroids_gjson_prefix+scene_id+'.geojson'\n",
    "    \n",
    "    # Skip if file exists and we don't want to overwrite it\n",
    "    if os.path.isfile(output_gjson) and not overwrite:\n",
    "        print('Skipping scene ', scene_id, ' (file exists)')\n",
    "    else:\n",
    "    \n",
    "        # Get catalog entries for specified scene\n",
    "        site_cat_i = site_cat_gdf[site_cat_gdf.eod_grid_id == scene_id]\n",
    "\n",
    "        # Create chips within scene\n",
    "        site_chip_unq = spark.read.chip(site_cat_i, ['BQA'],\n",
    "                            chipping_strategy=chp.SceneAlignedGrid(chip_size, chip_size)) \\\n",
    "                      .select('index', 'eod_grid_id', 'BQA') \\\n",
    "                      .withColumn('tot_cell_count', rf_data_cells('BQA')) \\\n",
    "                      .filter(F.col('tot_cell_count') == chip_size*chip_size) \\\n",
    "                      .withColumn('BQA_min', rf_tile_min('BQA')) \\\n",
    "                      .filter(F.col('BQA_min') > 1.0) \\\n",
    "                      .withColumn('tile_extent_4326', st_reproject(st_geometry(rf_extent('BQA')), \n",
    "                                                                   rf_crs('BQA'), lit('EPSG:4326'))) \\\n",
    "                      .drop('BQA', 'tot_cell_count', 'BQA_min') \\\n",
    "                      .distinct()\n",
    "        \n",
    "        # Load into pandas data frame\n",
    "        site_chip_pdf = site_chip_unq.toPandas()\n",
    "        chp_cnt = len(site_chip_pdf)\n",
    "                    \n",
    "        # Skip if data frame is empty\n",
    "        if chp_cnt == 0:\n",
    "            print('Skipping scene ', scene_id, ' (no coverage)')\n",
    "        else:            \n",
    "            \n",
    "            # Create unique chip id\n",
    "            tile_id = [scene_id+'-'+str(row).zfill(5) for row in list(range(1,chp_cnt+1))]\n",
    "    \n",
    "            # Write out chips extents to GeoJSON file\n",
    "            tile_geom_gdf = gpd.GeoDataFrame({'region_id': site_chip_pdf['index'],\n",
    "                                  'scene_id': site_chip_pdf.eod_grid_id,\n",
    "                                  'tile_id': tile_id,\n",
    "                                  'tile_extent': site_chip_pdf.tile_extent_4326},\n",
    "                                  geometry='tile_extent',\n",
    "                                  crs='EPSG:4326')\n",
    "            tile_geom_gdf.to_file(output_path+chip_extents_gjson_prefix+scene_id+'.geojson', driver='GeoJSON')\n",
    "    \n",
    "            # Find chip centroids and write to GeoJSON\n",
    "            tile_centroid_gdf = tile_geom_gdf\n",
    "            tile_centroid_gdf['tile_cntr'] = tile_centroid_gdf.geometry.centroid\n",
    "            tile_centroid_gdf = tile_centroid_gdf.set_geometry('tile_cntr').drop('tile_extent', axis=1)\n",
    "            tile_centroid_gdf.to_file(output_gjson, driver='GeoJSON')\n",
    "    \n",
    "            print('Done creating ', chp_cnt, ' chip centroids for scene ', scene_id, \n",
    "              ' (', i+1, ' out of ', l8_scene_cnt, ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tar up files and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chip centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_cntr_files = glob.glob(output_path+'*centroids*.geojson')\n",
    "chip_cntr_subdir = output_path+chip_centroids_gjson_prefix.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(chip_cntr_subdir):\n",
    "    os.mkdir(chip_cntr_subdir)\n",
    "[shutil.move(f, chip_cntr_subdir) for f in chip_cntr_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -C '+output_path+' -cvf '+chip_cntr_subdir.split('/')[-1]+'.tar '+chip_cntr_subdir.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')\n",
    "\n",
    "bucket.upload_file(chip_cntr_subdir.split('/')[-1]+'.tar', \n",
    "                   s3_path+'/'+chip_cntr_subdir.split('/')[-1]+'.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chip extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_ext_files = glob.glob(output_path+'*extents*.geojson')\n",
    "chip_ext_subdir = output_path+chip_extents_gjson_prefix.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(chip_ext_subdir):\n",
    "    os.mkdir(chip_ext_subdir)\n",
    "[shutil.move(f, chip_ext_subdir) for f in chip_ext_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -C '+output_path+' -cvf '+chip_ext_subdir.split('/')[-1]+'.tar '+chip_ext_subdir.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.upload_file(chip_ext_subdir.split('/')[-1]+'.tar', \n",
    "                   s3_path+'/'+chip_ext_subdir.split('/')[-1]+'.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up large files on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(chip_cntr_subdir.split('/')[-1]+'.tar')\n",
    "os.remove(chip_ext_subdir.split('/')[-1]+'.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
