{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Statitics for TIR Landsat 8 Macrolocalization Model Deployment\n",
    "\n",
    "* Cumulative percent of scored tiles by deployment date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input/output files and paths, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `years_sets` zip that defines the years and subsets for model deployment to consider\n",
    "* `pred_thresh` is the prediction threshold for selecting deployment grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_sets = zip(['2020', '2020', '2019', '2019', '2018', '2018'],\n",
    "                 ['1', '2', '1', '2', '1', '2'])\n",
    "pred_thresh = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to run\n",
    "\n",
    "So we don't repeat analysis we've already completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cum_frac = False\n",
    "compile_model_scores = False\n",
    "create_score_hists = False\n",
    "exam_score_hists = False\n",
    "exam_fusion_options = False\n",
    "compute_cmt_dist = False\n",
    "exam_cmt_dist = True\n",
    "compute_stl_dist = False\n",
    "exam_stl_dist = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files and paths\n",
    "\n",
    "* `s3_path` defines S3 high-level folder for L8 TIR macro-localization data\n",
    "* `chip_ext_tar` is the tar with GeoJSON files of chip extens for the deployment region\n",
    "* `score_tar_prefix` define prefix of tar files of score GeoJSONS\n",
    "* `acct_csv_prefix` prefix of location of scene accounting csv files\n",
    "* `macro_10km_shp` is a shapefile specifying the 10km grid from the proximity to infrastructure model\n",
    "* `LOCAL_DIR` specifies where to keep put files locally for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 'L8-TIR-macro-localization-model-deployment'\n",
    "chip_ext_tar = 'L8-deployment-chip-extents-CHN-10km-pthsh0.002.tar'\n",
    "score_tar_prefix = 'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'\n",
    "acct_csv_prefix = '../../resources/macro-loc-model-deployment/L8-deployment-scene_acct-CHN-10km-pthsh0.002'\n",
    "\n",
    "cement_site_geojson = \"../../resources/macro-loc-model-build/cement_exact_china_v4.1.geojson\"\n",
    "steel_site_geojson = \"../../resources/macro-loc-model-build/steel_exact_china_v4.1.geojson\"\n",
    "\n",
    "macro_10km_shp = \"../../resources/nt-model/10km_CS_macro/macroloc_cement_steel_CHN_10.shp\"\n",
    "\n",
    "knwn_cntr_scores_gjson = '../../resources/macro-loc-model-deployment/L8-known-plant-chip-fastai-scores-CHN-10km-pthsh0.002_2020.geojson'\n",
    "\n",
    "LOCAL_DIR = '/scratch/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `scene_acct_png` is png showing bar chart of scoring effort\n",
    "* `prob_hist_csv`\n",
    "* `known_prob_hist_csv`\n",
    "* `cmt_hist_png`\n",
    "* `cmt_cum_hist_png`\n",
    "* `stl_hist_png`\n",
    "* `stl_cum_hist_png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_acct_png = acct_csv_prefix+'_bar_chart.png'\n",
    "\n",
    "prob_hist_csv = acct_csv_prefix.replace('scene_acct', 'score_prob_hist')+'.csv'\n",
    "compiled_scores_csv = (acct_csv_prefix.replace('scene_acct', 'compiled_scores')+'.csv').split('/')[-1]\n",
    "\n",
    "cmt_hist_png = acct_csv_prefix.replace('scene_acct', 'cement_score_hist')+'.png'\n",
    "cmt_cum_hist_png = acct_csv_prefix.replace('scene_acct', 'cement_score_cum_hist')+'.png'\n",
    "stl_hist_png = acct_csv_prefix.replace('scene_acct', 'steel_score_hist')+'.png'\n",
    "stl_cum_hist_png = acct_csv_prefix.replace('scene_acct', 'steel_score_cum_hist')+'.png'\n",
    "\n",
    "cmt_comb_prob_scatter_png = acct_csv_prefix.replace('scene_acct', 'cement_comb_score_scatter')+'.png'\n",
    "cmt_comb_prob_hist_png = acct_csv_prefix.replace('scene_acct', 'cement_comb_score_hist')+'.png'\n",
    "cmt_knwn_comb_hist_png = acct_csv_prefix.replace('scene_acct', 'cement_knwn_comb_score_hist')+'.png'\n",
    "\n",
    "stl_comb_prob_scatter_png = acct_csv_prefix.replace('scene_acct', 'steel_comb_score_scatter')+'.png'\n",
    "stl_comb_prob_hist_png = acct_csv_prefix.replace('scene_acct', 'steel_comb_score_hist')+'.png'\n",
    "stl_knwn_comb_hist_png = acct_csv_prefix.replace('scene_acct', 'steel_knwn_comb_score_hist')+'.png'\n",
    "\n",
    "cmt_dist_csv = acct_csv_prefix.replace('scene_acct', 'cement_knwn_chip_dist')+'.csv'\n",
    "cmt_dist_2dhist_png = cmt_dist_csv.replace('chip_dist','dist_2dhist').replace('.csv','.png')\n",
    "cmt_dist_1dhist_png = cmt_dist_csv.replace('chip_dist','dist_1dhist').replace('.csv','.png')\n",
    "cmt_chp_cntr_comp_png = cmt_dist_csv.replace('chip_dist','cntr_score_compare').replace('.csv','.png')\n",
    "\n",
    "stl_dist_csv = acct_csv_prefix.replace('scene_acct', 'steel_knwn_chip_dist')+'.csv'\n",
    "stl_dist_2dhist_png = stl_dist_csv.replace('chip_dist','dist_2dhist').replace('.csv','.png')\n",
    "stl_dist_1dhist_png = stl_dist_csv.replace('chip_dist','dist_1dhist').replace('.csv','.png')\n",
    "stl_chp_cntr_comp_png = stl_dist_csv.replace('chip_dist','cntr_score_compare').replace('.csv','.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cumulative Count of Scored Chips by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_cum_frac:\n",
    "\n",
    "    # Read in and merge scene accounting files for set 1 and set 2\n",
    "    set1_acct_pdf = pd.read_csv(acct_csv_prefix+'set1.csv', index_col=False)\n",
    "    set2_acct_pdf = pd.read_csv(acct_csv_prefix+'set2.csv', index_col=False)\n",
    "    scene_acct_pdf = pd.concat([set1_acct_pdf, set2_acct_pdf], ignore_index=True)\n",
    "    \n",
    "    # Compute cumulative counts\n",
    "    scene_acct_pdf['tile_cnt_2019_cum'] = scene_acct_pdf.tile_cnt_2020 + scene_acct_pdf.tile_cnt_2019\n",
    "    scene_acct_pdf['tile_cnt_2018_cum'] = scene_acct_pdf.tile_cnt_2019_cum + scene_acct_pdf.tile_cnt_2018\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    bar_x = ['2020', '2019', '2018']\n",
    "    bar_y = [scene_acct_pdf.tile_cnt_2020.sum(axis=0),\n",
    "         scene_acct_pdf.tile_cnt_2019.sum(axis=0),\n",
    "         scene_acct_pdf.tile_cnt_2018.sum(axis=0)]\n",
    "    bar_y_cum = [scene_acct_pdf.tile_cnt_2020.sum(axis=0),\n",
    "             scene_acct_pdf.tile_cnt_2019_cum.sum(axis=0),\n",
    "             scene_acct_pdf.tile_cnt_2018_cum.sum(axis=0)]\n",
    "    acct_bar_chrt_pdf = pd.DataFrame({'Scored Chips Count': bar_y,\n",
    "                                  'Cumulative Scored Chips Count': bar_y_cum},\n",
    "                                 index = bar_x)\n",
    "    \n",
    "    # Plot and save figure\n",
    "    acct_bar_plot = acct_bar_chrt_pdf.plot.bar(rot=0).get_figure()\n",
    "    plt.title('Landsat-8 TIR Model Deployment Summary')\n",
    "    plt.xlabel('Deployment Year')\n",
    "    plt.ylabel('Number of Chips Scored')\n",
    "    plt.axhline(scene_acct_pdf.tile_cnt_tot.sum(axis=0), xmin=0.75, label='Total Chip Count', color='tab:orange')\n",
    "    plt.legend()\n",
    "    acct_bar_plot.savefig(scene_acct_png)\n",
    "    \n",
    "    # Print summary table of cumulative score effort\n",
    "    acct_bar_chrt_pdf['Fraction Chips Scored'] = acct_bar_chrt_pdf['Scored Chips Count'] / scene_acct_pdf.tile_cnt_tot.sum(axis=0)\n",
    "    acct_bar_chrt_pdf['Cumulative Fraction Chips Scored'] = acct_bar_chrt_pdf['Cumulative Scored Chips Count'] / scene_acct_pdf.tile_cnt_tot.sum(axis=0)\n",
    "    print('Total number of chips: ', scene_acct_pdf.tile_cnt_tot.sum(axis=0))\n",
    "    print(acct_bar_chrt_pdf[['Cumulative Scored Chips Count', 'Cumulative Fraction Chips Scored']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile model scores from all chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compile_model_scores:\n",
    "    \n",
    "    # Loop over all years and sets\n",
    "    for yr, st in years_sets:\n",
    "        \n",
    "        # Download tar file from S3 and untar\n",
    "        score_tar_file = score_tar_prefix+yr+'_set'+st+'.tar'\n",
    "        bucket.download_file(s3_path+'/'+score_tar_file, LOCAL_DIR+score_tar_file)\n",
    "        !tar -xf {LOCAL_DIR+score_tar_file} -C {LOCAL_DIR}\n",
    "        print(\"Finished downloading and extracting \", score_tar_file)\n",
    "    \n",
    "        # Read in GeoJSONs of known cement and steel plants\n",
    "        cement_site_gdf = gpd.read_file(cement_site_geojson)\n",
    "        steel_site_gdf = gpd.read_file(steel_site_geojson)\n",
    "        \n",
    "        # Read in shapefile of deployment grid, filter by pred_thresh\n",
    "        macro_10km_gdf = gpd.read_file(macro_10km_shp)\n",
    "        macro_10km_gdf = macro_10km_gdf[macro_10km_gdf.preds >= pred_thresh]\n",
    "    \n",
    "        # Get list of GeoJSONS\n",
    "        score_dir = score_tar_file.replace('.tar', '')\n",
    "        score_gjsons = glob.glob(LOCAL_DIR+score_dir+'/*.geojson')\n",
    "        score_gjsons.sort()\n",
    "        \n",
    "        # Loop over all GeoJSONS\n",
    "        for gf in score_gjsons:\n",
    "            \n",
    "            # Read in DataFrame\n",
    "            score_gdf = gpd.read_file(gf)\n",
    "            \n",
    "            # Join 10km preds; take max if tile intersects two grid sections\n",
    "            score_gdf = gpd.sjoin(score_gdf, macro_10km_gdf, how='left', \n",
    "                                  op='intersects')\n",
    "            score_gdf = score_gdf.drop(['scene_id', 'index_right', 'length', \n",
    "                                        'length_w', 'Count_pnt'], axis=1)\n",
    "            score_gdf['grpid'] = score_gdf['tile_id']\n",
    "            score_gdf = score_gdf.sort_values('preds', ascending=False) \\\n",
    "                                 .groupby(['grpid']).first()\n",
    "            \n",
    "            # Join to known cement plant sites\n",
    "            score_gdf = gpd.sjoin(score_gdf, cement_site_gdf, how='left', \n",
    "                                  op='intersects')\n",
    "            score_gdf['cement_uid'] = score_gdf.uid\n",
    "            score_gdf = score_gdf.drop(['index_right', 'uid'], axis=1)\n",
    "            \n",
    "            # Join to known steel plant sites\n",
    "            score_gdf = gpd.sjoin(score_gdf, steel_site_gdf, how='left', \n",
    "                                  op='intersects')\n",
    "            score_gdf['steel_uid'] = score_gdf.uid\n",
    "            score_gdf = score_gdf.drop(['index_right', 'uid'], axis=1)\n",
    "        \n",
    "            print('Done compiling scores for ', len(score_gdf), ' chips in ', gf)\n",
    "            \n",
    "            # Append to compiled scores file, droping geom column\n",
    "            if 'compiled_scores_pdf' in locals():\n",
    "                compiled_scores_pdf = pd.concat([compiled_scores_pdf, \n",
    "                                                 score_gdf.drop('geometry', axis=1)], \n",
    "                                                ignore_index=True)\n",
    "            else:\n",
    "                compiled_scores_pdf = score_gdf.drop('geometry', axis=1)\n",
    "            \n",
    "    # Save results in csv\n",
    "    compiled_scores_pdf.to_csv(compiled_scores_csv, index=False)\n",
    "    \n",
    "    # Upload to S3 (too bit for git)\n",
    "    bucket.upload_file(compiled_scores_csv, s3_path+'/'+compiled_scores_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create histograms of probabilities of model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_score_hists:\n",
    "    \n",
    "    # Read in combined score results\n",
    "    compiled_scores_pdf = pd.read_csv(compiled_scores_csv, index_col=False,\n",
    "                                     low_memory=False)\n",
    "    \n",
    "    # Define bin edges and probabability histograms\n",
    "    bedges = np.arange(0, 1.01, 0.01)\n",
    "    \n",
    "    # All Chips (drop duplicates where >1 plant intersects)\n",
    "    # ---------\n",
    "    comp_scores_nodups = compiled_scores_pdf.drop_duplicates(subset='tile_id', \n",
    "                                                             keep='first')\n",
    "    # Cement\n",
    "    cmt_prb_hist, bin_edges = np.histogram(comp_scores_nodups.cement_prob, \n",
    "                                           bins=bedges)\n",
    "    # Steel\n",
    "    stl_prb_hist, bin_edges = np.histogram(comp_scores_nodups.steel_prob, \n",
    "                                           bins=bedges)\n",
    "    \n",
    "    # Chips intersecting with known plants\n",
    "    # ------------------------------------\n",
    "    # Cement\n",
    "    cmt_prob_knwn = compiled_scores_pdf[~compiled_scores_pdf.cement_uid.isnull()]\n",
    "    cmt_knwn_prb_hist, bin_edges = np.histogram(cmt_prob_knwn.cement_prob, \n",
    "                                                bins=bedges)\n",
    "    # Steel\n",
    "    stl_prob_knwn = compiled_scores_pdf[~compiled_scores_pdf.steel_uid.isnull()]\n",
    "    stl_knwn_prb_hist, bin_edges = np.histogram(stl_prob_knwn.steel_prob, \n",
    "                                                bins=bedges)\n",
    "            \n",
    "    # Save results in DataFrame and write to csv\n",
    "    model_prob_hist = pd.DataFrame({'bin_left_edge': bin_edges[:-1],\n",
    "                                    'cmt_prb_hist': cmt_prb_hist,\n",
    "                                    'cmt_knwn_prb_hist': cmt_knwn_prb_hist,\n",
    "                                    'stl_prb_hist': stl_prb_hist,\n",
    "                                    'stl_knwn_prb_hist': stl_knwn_prb_hist})\n",
    "    model_prob_hist.to_csv(prob_hist_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine scores of known sites compared to all chip scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exam_score_hists:\n",
    "    \n",
    "    # Read in histograms\n",
    "    model_prob_hist = pd.read_csv(prob_hist_csv, index_col=False)\n",
    "    \n",
    "    # Plot histograms for cement plants\n",
    "    # ---------------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharey=False)\n",
    "\n",
    "    ax1.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.cmt_prb_hist,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        label='All Chips')\n",
    "    ax1.set(title='Cement Model Probability Distribution', ylabel='Count')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.cmt_knwn_prb_hist,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        color='tab:orange',\n",
    "        label='Chips Intersecting Known Plants')\n",
    "    ax2.set(ylabel='Count', xlabel='Model Probability')\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.savefig(cmt_hist_png)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot histograms for steel plants\n",
    "    # ---------------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharey=False)\n",
    "\n",
    "    ax1.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.stl_prb_hist,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        label='All Chips')\n",
    "    ax1.set(title='Steel Model Probability Distribution', ylabel='Count')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.stl_knwn_prb_hist,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        color='tab:orange',\n",
    "        label='Chips Intersecting Known Plants')\n",
    "    ax2.set(ylabel='Count', xlabel='Model Probability')\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.savefig(stl_hist_png)\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute cumulative histograms\n",
    "    model_prob_hist['cmt_prb_cumsum'] = model_prob_hist.cmt_prb_hist.cumsum()\n",
    "    model_prob_hist['stl_prb_cumsum'] = model_prob_hist.stl_prb_hist.cumsum()\n",
    "    model_prob_hist['cmt_knwn_prb_cumsum'] = model_prob_hist.cmt_knwn_prb_hist.cumsum()\n",
    "    model_prob_hist['stl_knwn_prb_cumsum'] = model_prob_hist.stl_knwn_prb_hist.cumsum()\n",
    "    \n",
    "    # Plot cumulative histograms for cement\n",
    "    # ------------------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharey=False)\n",
    "\n",
    "    ax1.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.cmt_prb_cumsum,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        label='All Chips')\n",
    "    ax1.set(title='Cement Model Probability Cumulative Distribution', \n",
    "        ylabel='Cumulative Count')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.cmt_knwn_prb_cumsum,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        color='tab:orange',\n",
    "        label='Chips Intersecting Known Plants')\n",
    "    ax2.set(ylabel='Cumulative Count', xlabel='Model Probability')\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.savefig(cmt_cum_hist_png)\n",
    "    plt.show()\n",
    "    \n",
    "    cmt_thr1000 = model_prob_hist.loc[(model_prob_hist.cmt_prb_cumsum - \n",
    "                            (model_prob_hist.cmt_prb_cumsum.max()-1000)).abs().idxmin()]\n",
    "    print('Cement probability threshold to grab top 1000:', cmt_thr1000.bin_left_edge)\n",
    "    \n",
    "    # Plot cumulative histograms for steel\n",
    "    # ------------------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharey=False)\n",
    "\n",
    "    ax1.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.stl_prb_cumsum,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        label='All Chips')\n",
    "    ax1.set(title='Steel Model Probability Cumulative Distribution', \n",
    "        ylabel='Cumulative Count')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(x=model_prob_hist.bin_left_edge,\n",
    "        height=model_prob_hist.stl_knwn_prb_cumsum,\n",
    "        align='edge',\n",
    "        width=0.01,\n",
    "        color='tab:orange',\n",
    "        label='Chips Intersecting Known Plants')\n",
    "    ax2.set(ylabel='Cumulative Count', xlabel='Model Probability')\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.savefig(stl_cum_hist_png)\n",
    "    plt.show()\n",
    "    \n",
    "    stl_thr1000 = model_prob_hist.loc[(model_prob_hist.stl_prb_cumsum - \n",
    "                            (model_prob_hist.stl_prb_cumsum.max()-1000)).abs().idxmin()]\n",
    "    print('Steel probability threshold to grab top 1000:', stl_thr1000.bin_left_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate options for L8 and Proximity Model fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exam_fusion_options:\n",
    "    \n",
    "    # Read in combined score results\n",
    "    compiled_scores_pdf = pd.read_csv(compiled_scores_csv, index_col=False,\n",
    "                                      low_memory=False)\n",
    "    \n",
    "    # Bin data into quantiles accroding to pred\n",
    "    qbins = [0, 0.25, 0.5, 0.75, 1]\n",
    "    compiled_scores_pdf['pred_grp'] = pd.qcut(compiled_scores_pdf.preds, q=qbins)#, labels=qgrp)\n",
    "    \n",
    "    # drop dups for pop averages\n",
    "    comp_scores_nodups = compiled_scores_pdf.drop_duplicates(subset='tile_id', \n",
    "                                                             keep='first')\n",
    "    # Compute average L8 probabilities per pred\n",
    "    avg_prob_vs_pred = comp_scores_nodups[['index','preds','cement_prob','steel_prob']].groupby(['index']).mean()  \n",
    "    \n",
    "    # Cement\n",
    "    # ------\n",
    "    \n",
    "    # Intersection w/ known cement plants\n",
    "    cmt_prob_knwn = compiled_scores_pdf[~compiled_scores_pdf.cement_uid.isnull()]\n",
    "    \n",
    "    # Scatter plot for cement\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.plot(comp_scores_nodups.preds, comp_scores_nodups.cement_prob, \n",
    "         color='silver', marker='.', linestyle='', label='All Chips')\n",
    "    plt.plot(avg_prob_vs_pred.preds, avg_prob_vs_pred.cement_prob, \n",
    "         color='steelblue', marker='x', linestyle='', label='Average (All Chips)')\n",
    "    plt.plot(cmt_prob_knwn.preds, cmt_prob_knwn.cement_prob, \n",
    "         color='seagreen', marker='x', linestyle='', label='Known Plants')\n",
    "    plt.xlim([0,0.3])\n",
    "    plt.xlabel('Probability (Proximity Model)')\n",
    "    plt.ylabel('Probability (Landsat 8 TIR Model)')\n",
    "    plt.legend()\n",
    "    plt.title('Cement Model Probabilities')\n",
    "    fig.savefig(cmt_comb_prob_scatter_png)\n",
    "    \n",
    "    # L8 scores by pred quantile - All chips\n",
    "    comp_grp_scores = comp_scores_nodups.groupby('pred_grp')\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ecol = ['k', 'b', 'g', 'r']\n",
    "    labs = [str(f) for f in list(comp_grp_scores.groups.keys())]\n",
    "    i = 0\n",
    "    for group in comp_grp_scores:\n",
    "        plt.hist(group[1].cement_prob, bins=50,\n",
    "                 color=ecol[i], edgecolor=ecol[i],\n",
    "                 label=labs[i], histtype='step')\n",
    "        i = i+1\n",
    "    plt.legend(title='Proximity Probability Interval')\n",
    "    plt.xlabel('Landsat 8 TIR Model Probability')\n",
    "    plt.ylabel('Count (All Chips)')\n",
    "    plt.title('Cement Model Probability Distribution')\n",
    "    plt.xlim([0,1])\n",
    "    fig.savefig(cmt_comb_prob_hist_png)\n",
    "    \n",
    "    # L8 scores by pred quantile - Intersecting with plants\n",
    "    cmt_prob_knwn_grps = cmt_prob_knwn.groupby('pred_grp')\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ecol = ['k', 'b', 'g', 'r']\n",
    "    labs = [str(f) for f in list(cmt_prob_knwn_grps.groups.keys())]\n",
    "    i = 0\n",
    "    for group in cmt_prob_knwn_grps:\n",
    "        plt.hist(group[1].cement_prob, bins=10,\n",
    "                 color=ecol[i], edgecolor=ecol[i],\n",
    "                 label=labs[i], histtype='step')\n",
    "        i = i+1\n",
    "    plt.legend(title='Proximity Probability Interval')\n",
    "    plt.xlabel('Landsat 8 TIR Model Probability')\n",
    "    plt.ylabel('Count (Chips Intersecting Known Plants)')\n",
    "    plt.title('Cement Model Probability Distribution')\n",
    "    plt.xlim([0,1])\n",
    "    fig.savefig(cmt_knwn_comb_hist_png)\n",
    "    \n",
    "    print('Number of Cement Plants per Quantile:')\n",
    "    print(cmt_prob_knwn['pred_grp'].value_counts())\n",
    "    \n",
    "    # Steel\n",
    "    # ------\n",
    "    \n",
    "    # Intersection w/ known steel plants\n",
    "    stl_prob_knwn = compiled_scores_pdf[~compiled_scores_pdf.steel_uid.isnull()]\n",
    "    \n",
    "    # Scatter plot for steel\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.plot(comp_scores_nodups.preds, comp_scores_nodups.steel_prob, \n",
    "         color='silver', marker='.', linestyle='', label='All Chips')\n",
    "    plt.plot(avg_prob_vs_pred.preds, avg_prob_vs_pred.steel_prob, \n",
    "         color='steelblue', marker='x', linestyle='', label='Average (All Chips)')\n",
    "    plt.plot(stl_prob_knwn.preds, stl_prob_knwn.steel_prob, \n",
    "         color='seagreen', marker='x', linestyle='', label='Known Plants')\n",
    "    plt.xlim([0,0.3])\n",
    "    plt.xlabel('Probability (Proximity Model)')\n",
    "    plt.ylabel('Probability (Landsat 8 TIR Model)')\n",
    "    plt.legend()\n",
    "    plt.title('Steel Model Probabilities')\n",
    "    fig.savefig(stl_comb_prob_scatter_png)\n",
    "    \n",
    "    # L8 scores by pred quantile - All chips\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ecol = ['k', 'b', 'g', 'r']\n",
    "    labs = [str(f) for f in list(comp_grp_scores.groups.keys())]\n",
    "    i = 0\n",
    "    for group in comp_grp_scores:\n",
    "        plt.hist(group[1].steel_prob, bins=50,\n",
    "                 color=ecol[i], edgecolor=ecol[i],\n",
    "                 label=labs[i], histtype='step')\n",
    "        i = i+1\n",
    "    plt.legend(title='Proximity Probability Interval')\n",
    "    plt.xlabel('Landsat 8 TIR Model Probability')\n",
    "    plt.ylabel('Count (All Chips)')\n",
    "    plt.title('Steel Model Probability Distribution')\n",
    "    plt.xlim([0,1])\n",
    "    fig.savefig(stl_comb_prob_hist_png)\n",
    "    \n",
    "    # L8 scores by pred quantile - Intersecting with plants\n",
    "    stl_prob_knwn_grps = stl_prob_knwn.groupby('pred_grp')\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ecol = ['k', 'b', 'g', 'r']\n",
    "    labs = [str(f) for f in list(stl_prob_knwn_grps.groups.keys())]\n",
    "    i = 0\n",
    "    for group in stl_prob_knwn_grps:\n",
    "        plt.hist(group[1].steel_prob, bins=10,\n",
    "                 color=ecol[i], edgecolor=ecol[i],\n",
    "                 label=labs[i], histtype='step')\n",
    "        i = i+1\n",
    "    plt.legend(title='Proximity Probability Interval')\n",
    "    plt.xlabel('Landsat 8 TIR Model Probability')\n",
    "    plt.ylabel('Count (Chips Intersecting Known Plants)')\n",
    "    plt.title('Steel Model Probability Distribution')\n",
    "    plt.xlim([0,1])\n",
    "    fig.savefig(stl_knwn_comb_hist_png)\n",
    "    \n",
    "    print('Number of Steel Plants per Quantile:')\n",
    "    print(stl_prob_knwn['pred_grp'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Study whether distance from chip center to plant affects cement results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_cmt_dist:\n",
    "    \n",
    "    # Read in combined score results\n",
    "    bucket.download_file(s3_path+'/'+compiled_scores_csv, LOCAL_DIR+compiled_scores_csv)\n",
    "    compiled_scores_pdf = pd.read_csv(LOCAL_DIR+compiled_scores_csv, index_col=False,\n",
    "                                      low_memory=False)\n",
    "    \n",
    "    # Limit to chips covering known cement plants\n",
    "    cmt_prob_knwn = compiled_scores_pdf[~compiled_scores_pdf.cement_uid.isnull()] \\\n",
    "                                .drop(['index', 'prop_rail', 'prop_water', 'preds',\n",
    "                                       'steel_prob', 'steel_uid'], axis=1)\n",
    "    \n",
    "    # Define groups to find right GeoJson files\n",
    "    cmt_prob_knwn['scene_id'] = ['-'.join(f.split('-')[0:2]) for f in cmt_prob_knwn.tile_id]\n",
    "    cmt_prob_knwn['set_id'] = ['set1' if int(sid.split('-')[-1][0:3]) <= 125 else 'set2' for sid in cmt_prob_knwn['scene_id']]\n",
    "    cmt_prob_knwn_grps = cmt_prob_knwn.groupby(['year', 'set_id', 'scene_id'])\n",
    "    grps_to_read = list(cmt_prob_knwn_grps.groups.keys())\n",
    "    \n",
    "    # Loop over all years and sets and download tar file from S3 and untar\n",
    "    for yr, st in years_sets:\n",
    "        score_tar_file = score_tar_prefix+yr+'_set'+st+'.tar'\n",
    "        bucket.download_file(s3_path+'/'+score_tar_file, LOCAL_DIR+score_tar_file)\n",
    "        !tar -xf {LOCAL_DIR+score_tar_file} -C {LOCAL_DIR}\n",
    "        print(\"Finished downloading and extracting \", score_tar_file)\n",
    "        \n",
    "    # Read in GeoJSONs of known cement plants\n",
    "    cement_site_gdf = gpd.read_file(cement_site_geojson)\n",
    "    \n",
    "    # Loop over scene/year/set groups to compile distance data\n",
    "    calc_crs = \"EPSG:3395\"\n",
    "    for group in cmt_prob_knwn_grps:\n",
    "    \n",
    "        # Get filename of GeoJson\n",
    "        score_dir = LOCAL_DIR+'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+str(group[0][0])+'_'+group[0][1]\n",
    "        score_gjson = score_dir+'/'+'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+str(group[0][0])+'_'+group[0][2]+'.geojson'\n",
    "        print(score_gjson)\n",
    "        \n",
    "        # Read in GeoJson file\n",
    "        score_gdf = gpd.read_file(score_gjson)\n",
    "    \n",
    "        # Merge to data group to get geometry, compute centroid of chip,\n",
    "        # and convert to physical crs\n",
    "        score_gdf = score_gdf[['tile_id', 'geometry']].merge(group[1], \n",
    "                                                         on='tile_id', \n",
    "                                                         how='right')\n",
    "        score_gdf['geometry'] = score_gdf.geometry.centroid\n",
    "        score_gdf = score_gdf.to_crs(calc_crs)\n",
    "    \n",
    "        # Merge known plant points to data group, convert crs\n",
    "        knwn_plnt_gdf = cement_site_gdf.merge(group[1], left_on='uid', \n",
    "                                          right_on='cement_uid', how='right')\n",
    "        knwn_plnt_gdf = knwn_plnt_gdf.to_crs(calc_crs)\n",
    "    \n",
    "        # Be safe, make sure we got the rows aligned\n",
    "        vals_match = (score_gdf.tile_id == knwn_plnt_gdf.tile_id) & \\\n",
    "                     (score_gdf.cement_prob == knwn_plnt_gdf.cement_prob) & \\\n",
    "                     (score_gdf.cement_uid == knwn_plnt_gdf.cement_uid)\n",
    "        if sum(vals_match) != len(vals_match):\n",
    "            print('Oh no! Something is wrong. Work harder.')\n",
    "            break\n",
    "    \n",
    "        # Calculate distance and drop unneeded columns\n",
    "        score_gdf['dist_m'] = score_gdf.distance(knwn_plnt_gdf)\n",
    "        score_gdf = score_gdf.drop(['geometry', 'year', 'scene_id', 'set_id'], axis=1)\n",
    "     \n",
    "        # Append to full list of all known plants\n",
    "        if 'cmt_chp_plt_dist_pdf' in locals():\n",
    "            cmt_chp_plt_dist_pdf = pd.concat([cmt_chp_plt_dist_pdf, score_gdf], \n",
    "                                             ignore_index=True)\n",
    "        else:\n",
    "            cmt_chp_plt_dist_pdf = score_gdf\n",
    "    \n",
    "    # Save to csv\n",
    "    cmt_chp_plt_dist_pdf.to_csv(cmt_dist_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exam_cmt_dist:\n",
    "\n",
    "    # Read in chips distance file\n",
    "    cmt_chp_plt_dist_pdf = pd.read_csv(cmt_dist_csv, index_col=False)\n",
    "            \n",
    "    # 2D histogram of distance versus L8 score\n",
    "    fig, ax = plt.subplots(1)\n",
    "    h, xedge, yedge, im = plt.hist2d(cmt_chp_plt_dist_pdf.dist_m, cmt_chp_plt_dist_pdf.cement_prob, \n",
    "                                     bins=[10, 10],\n",
    "                                     range=[[0, 1000], [0, 1]])\n",
    "    plt.title('Cement Plants')\n",
    "    plt.xlabel('Distance between Chip Center and Cement Plant (m)')\n",
    "    plt.ylabel('Landsat 8 TIR Model Probability')\n",
    "    fig.savefig(cmt_dist_2dhist_png)\n",
    "    \n",
    "    # histogram of distances between chip centers and cement plants\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.bar(x=xedge[:-1],\n",
    "        height=h.sum(axis=1),\n",
    "        align='edge',\n",
    "        width=100)\n",
    "    plt.title('Cement Plants')\n",
    "    plt.ylabel('Count of Chips')\n",
    "    plt.xlabel('Distance between Chip Center and Cement Plant (m)')\n",
    "    plt.show()\n",
    "    fig.savefig(cmt_dist_1dhist_png)\n",
    "\n",
    "    # histogram of model probabilities (don't save - just to validate)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.bar(x=yedge[:-1],\n",
    "        height=h.sum(axis=0),\n",
    "        align='edge',\n",
    "        width=0.1)\n",
    "    plt.title('Cement Plants')\n",
    "    plt.ylabel('Count of Chips')\n",
    "    plt.xlabel('Landsat 8 TIR Model Probability')\n",
    "    plt.show()\n",
    "    \n",
    "    # Read in L8 model scores on chips centered on plants\n",
    "    knwn_cntr_scores_pdf = gpd.read_file(knwn_cntr_scores_gjson)\n",
    "    # Limit to cement plants\n",
    "    kwnw_cmt_cntr_pdf = knwn_cntr_scores_pdf[knwn_cntr_scores_pdf['site_type'] == 'cement']\n",
    "    kwnw_cmt_cntr_pdf['cement_cntr_prob'] = kwnw_cmt_cntr_pdf.cement_prob\n",
    "    \n",
    "    # Join to chip scores intersecrting with cement plants\n",
    "    kwnw_cmt_cntr_pdf = pd.merge(cmt_chp_plt_dist_pdf,\n",
    "                                 kwnw_cmt_cntr_pdf[['uid', 'cement_cntr_prob']],\n",
    "                                 how='left', right_on='uid', left_on='cement_uid')\n",
    "    \n",
    "    # Scatter plot of scores\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.plot(kwnw_cmt_cntr_pdf.cement_cntr_prob, kwnw_cmt_cntr_pdf.cement_prob, 'bx')\n",
    "    plt.title('Cement: Landsat 8 TIR Model Probabilty')\n",
    "    plt.xlabel('Chips Centered on Known Plants')\n",
    "    plt.ylabel('Intersecting Chips in Deployment Grid')\n",
    "    fig.savefig(cmt_chp_cntr_comp_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Study whether distance from chip center to plant affects steel results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_stl_dist:\n",
    "    \n",
    "    # Read in combined score results\n",
    "    bucket.download_file(s3_path+'/'+compiled_scores_csv, LOCAL_DIR+compiled_scores_csv)\n",
    "    compiled_scores_pdf = pd.read_csv(LOCAL_DIR+compiled_scores_csv, index_col=False,\n",
    "                                      low_memory=False)\n",
    "    \n",
    "    # Limit to chips covering known steel plants\n",
    "    stl_prob_knwn = compiled_scores_pdf[~compiled_scores_pdf.steel_uid.isnull()] \\\n",
    "                                .drop(['index', 'prop_rail', 'prop_water', 'preds',\n",
    "                                       'cement_prob', 'cement_uid'], axis=1)\n",
    "    \n",
    "    # Define groups to find right GeoJson files\n",
    "    stl_prob_knwn['scene_id'] = ['-'.join(f.split('-')[0:2]) for f in stl_prob_knwn.tile_id]\n",
    "    stl_prob_knwn['set_id'] = ['set1' if int(sid.split('-')[-1][0:3]) <= 125 else 'set2' for sid in stl_prob_knwn['scene_id']]\n",
    "    stl_prob_knwn_grps = stl_prob_knwn.groupby(['year', 'set_id', 'scene_id'])\n",
    "    grps_to_read = list(stl_prob_knwn_grps.groups.keys())\n",
    "    \n",
    "    # Loop over all years and sets and download tar file from S3 and untar\n",
    "    for yr, st in years_sets:\n",
    "        score_tar_file = score_tar_prefix+yr+'_set'+st+'.tar'\n",
    "        bucket.download_file(s3_path+'/'+score_tar_file, LOCAL_DIR+score_tar_file)\n",
    "        !tar -xf {LOCAL_DIR+score_tar_file} -C {LOCAL_DIR}\n",
    "        print(\"Finished downloading and extracting \", score_tar_file)\n",
    "        \n",
    "    # Read in GeoJSONs of known steel plants\n",
    "    steel_site_gdf = gpd.read_file(steel_site_geojson)\n",
    "    \n",
    "    # Loop over scene/year/set groups to compile distance data\n",
    "    calc_crs = \"EPSG:3395\"\n",
    "    for group in stl_prob_knwn_grps:\n",
    "    \n",
    "        # Get filename of GeoJson\n",
    "        score_dir = LOCAL_DIR+'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+str(group[0][0])+'_'+group[0][1]\n",
    "        score_gjson = score_dir+'/'+'L8-deployment-chip-scores-CHN-10km-pthsh0.002_'+str(group[0][0])+'_'+group[0][2]+'.geojson'\n",
    "        print(score_gjson)\n",
    "        \n",
    "        # Read in GeoJson file\n",
    "        score_gdf = gpd.read_file(score_gjson)\n",
    "    \n",
    "        # Merge to data group to get geometry, compute centroid of chip,\n",
    "        # and convert to physical crs\n",
    "        score_gdf = score_gdf[['tile_id', 'geometry']].merge(group[1], \n",
    "                                                         on='tile_id', \n",
    "                                                         how='right')\n",
    "        score_gdf['geometry'] = score_gdf.geometry.centroid\n",
    "        score_gdf = score_gdf.to_crs(calc_crs)\n",
    "    \n",
    "        # Merge known plant points to data group, convert crs\n",
    "        knwn_plnt_gdf = steel_site_gdf.merge(group[1], left_on='uid', \n",
    "                                          right_on='steel_uid', how='right')\n",
    "        knwn_plnt_gdf = knwn_plnt_gdf.to_crs(calc_crs)\n",
    "    \n",
    "        # Be safe, make sure we got the rows aligned\n",
    "        vals_match = (score_gdf.tile_id == knwn_plnt_gdf.tile_id) & \\\n",
    "                     (score_gdf.steel_prob == knwn_plnt_gdf.steel_prob) & \\\n",
    "                     (score_gdf.steel_uid == knwn_plnt_gdf.steel_uid)\n",
    "        if sum(vals_match) != len(vals_match):\n",
    "            print('Oh no! Something is wrong. Work harder.')\n",
    "            break\n",
    "    \n",
    "        # Calculate distance and drop unneeded columns\n",
    "        score_gdf['dist_m'] = score_gdf.distance(knwn_plnt_gdf)\n",
    "        score_gdf = score_gdf.drop(['geometry', 'year', 'scene_id', 'set_id'], axis=1)\n",
    "     \n",
    "        # Append to full list of all known plants\n",
    "        if 'stl_chp_plt_dist_pdf' in locals():\n",
    "            stl_chp_plt_dist_pdf = pd.concat([stl_chp_plt_dist_pdf, score_gdf], \n",
    "                                             ignore_index=True)\n",
    "        else:\n",
    "            stl_chp_plt_dist_pdf = score_gdf\n",
    "    \n",
    "    # Save to csv\n",
    "    stl_chp_plt_dist_pdf.to_csv(stl_dist_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exam_stl_dist:\n",
    "\n",
    "    # Read in chips distance file\n",
    "    stl_chp_plt_dist_pdf = pd.read_csv(stl_dist_csv, index_col=False)\n",
    "            \n",
    "    # 2D histogram of distance versus L8 score\n",
    "    fig, ax = plt.subplots(1)\n",
    "    h, xedge, yedge, im = plt.hist2d(stl_chp_plt_dist_pdf.dist_m, stl_chp_plt_dist_pdf.steel_prob, \n",
    "                                     bins=[10, 10],\n",
    "                                     range=[[0, 1000], [0, 1]])\n",
    "    plt.title('Steel Plants')\n",
    "    plt.xlabel('Distance between Chip Center and Steel Plant (m)')\n",
    "    plt.ylabel('Landsat 8 TIR Model Probability')\n",
    "    fig.savefig(stl_dist_2dhist_png)\n",
    "    \n",
    "    # histogram of distances between chip centers and steel plants\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.bar(x=xedge[:-1],\n",
    "        height=h.sum(axis=1),\n",
    "        align='edge',\n",
    "        width=100)\n",
    "    plt.title('Steel Plants')\n",
    "    plt.ylabel('Count of Chips')\n",
    "    plt.xlabel('Distance between Chip Center and Steel Plant (m)')\n",
    "    plt.show()\n",
    "    fig.savefig(stl_dist_1dhist_png)\n",
    "\n",
    "    # histogram of model probabilities (don't save - just to validate)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.bar(x=yedge[:-1],\n",
    "        height=h.sum(axis=0),\n",
    "        align='edge',\n",
    "        width=0.1)\n",
    "    plt.title('Steel Plants')\n",
    "    plt.ylabel('Count of Chips')\n",
    "    plt.xlabel('Landsat 8 TIR Model Probability')\n",
    "    plt.show()\n",
    "    \n",
    "    # Read in L8 model scores on chips centered on plants\n",
    "    knwn_cntr_scores_pdf = gpd.read_file(knwn_cntr_scores_gjson)\n",
    "    # Limit to steel plants\n",
    "    kwnw_stl_cntr_pdf = knwn_cntr_scores_pdf[knwn_cntr_scores_pdf['site_type'] == 'steel']\n",
    "    kwnw_stl_cntr_pdf['steel_cntr_prob'] = kwnw_stl_cntr_pdf.steel_prob\n",
    "    \n",
    "    # Join to chip scores intersecrting with steel plants\n",
    "    kwnw_stl_cntr_pdf = pd.merge(stl_chp_plt_dist_pdf,\n",
    "                                 kwnw_stl_cntr_pdf[['uid', 'steel_cntr_prob']],\n",
    "                                 how='left', right_on='uid', left_on='steel_uid')\n",
    "    \n",
    "    # Scatter plot of scores\n",
    "    fig, ax = plt.subplots(1)\n",
    "    plt.plot(kwnw_stl_cntr_pdf.steel_cntr_prob, kwnw_stl_cntr_pdf.steel_prob, 'bx')\n",
    "    plt.title('Steel: Landsat 8 TIR Model Probabilty')\n",
    "    plt.xlabel('Chips Centered on Known Plants')\n",
    "    plt.ylabel('Intersecting Chips in Deployment Grid')\n",
    "    fig.savefig(stl_chp_cntr_comp_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
