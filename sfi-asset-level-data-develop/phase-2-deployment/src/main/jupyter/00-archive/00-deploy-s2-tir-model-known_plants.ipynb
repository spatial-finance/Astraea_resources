{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment of RGB Sentinel-2 Macrolocalization Model - 2020\n",
    "\n",
    "This notebook deploys the RGB Sentinel-2 macrolocalization models for cement and steel plants for the year 2020 on known plants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==1.0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as pys\n",
    "from pyspark.sql.functions import lit, col, udf\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import glob\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "* Important to do this before defining the udfs for scoring\n",
    "* Set number of partitions on par with the number of catalog items per scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 2500\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input/output files and paths, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `unmsk_frac` is the minimum threshold on the fraction of unmasked cells required to keep site in sample\n",
    "* `year` defines the year of selected scenes\n",
    "* `month` defines the month of selected scenes (format: January = \"01\", Februaray = \"02\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_size = 300 # 3 km for Sentinel-2\n",
    "unmsk_frac = 0.75\n",
    "\n",
    "year = '2020'\n",
    "month = '06'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files and paths\n",
    "\n",
    "* `s3_path` defines S3 high-level folder for S2 RGB macro-localization data\n",
    "* `cement_site_geojson` is GeoJSON of cement plants with exact locations\n",
    "* `steel_site_geojson` is GeoJSON of steel plants with exact locations\n",
    "* `MODEL_PATH` is the path on S3 to the Densenet161 multiclass model\n",
    "* `LOCAL_DIR` specifies where to keep put files locally for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 'S2-RGB-macro-localization-model-deployment'\n",
    "\n",
    "cement_site_geojson = \"../../resources/macro-loc-model-build/cement_exact_china_v4.1.geojson\"\n",
    "steel_site_geojson = \"../../resources/macro-loc-model-build/steel_exact_china_v4.1.geojson\"\n",
    "\n",
    "MODEL_PATH = 'S2-RGB-macro-localization-model-build3/S2-RGB-model-results3/densenet161_multiclass_final.pkl'\n",
    "\n",
    "LOCAL_DIR = '/scratch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `output_score_file` define output GeoJSON of scores for known plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_score_file = '../../resources/macro-loc-model-deployment/S2-known-plant-chip-fastai-scores-CHN-10km-pthsh0.002_'+year+month+'.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model and Define Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model and load learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(MODEL_PATH):\n",
    "    if not os.path.exists(LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\")):\n",
    "        os.makedirs(LOCAL_DIR + MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "    bucket.download_file(MODEL_PATH, LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\") + \"/export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = load_learner(LOCAL_DIR + MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring function for PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pngs(path):\n",
    "    \n",
    "    # Get ImageDataBunch for Fastai\n",
    "    data = (ImageDataBunch.from_folder(path, train='all', bs=16, num_workers=0, seed=42).normalize(imagenet_stats))\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    data_cnt = len(data.train_ds)\n",
    "    uid = []\n",
    "    site_type = []\n",
    "    cement_prob = []\n",
    "    steel_prob = []\n",
    "    \n",
    "    # Loop over images and get scores and metadata\n",
    "    for i in range(0, data_cnt):\n",
    "        \n",
    "        # Model results\n",
    "        p_model = multi_model.predict(data.train_ds.x[i])\n",
    "        \n",
    "        # Cement probability\n",
    "        cement_prob.append(to_np(p_model[2])[0].item())\n",
    "    \n",
    "        # Steel probability\n",
    "        steel_prob.append(to_np(p_model[2])[2].item())\n",
    "    \n",
    "        # Metadata for chip\n",
    "        uid.append(str(data.items[i]).split('/')[-1].split('_')[0])\n",
    "        site_type.append(str(data.items[i]).split('_')[-1].split('.')[0])\n",
    "        \n",
    "    # Return data frame\n",
    "    score_pdf = pd.DataFrame({'uid': uid,\n",
    "                              'site_type': site_type,\n",
    "                              'cement_prob': cement_prob,\n",
    "                              'steel_prob': steel_prob})\n",
    "    \n",
    "    return(score_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EOD Catalog Read and Chipping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get catalog of Sentinel-2 scenes that intersect with chip centroids\n",
    "\n",
    "Queries EarthAI Catalog to find S2 scenes that intersect with chip centroids.\n",
    "\n",
    "* Returns specified scenes for year, month\n",
    "* Join back to chip centroids for chipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eod_read_catalog(geom, year, month):\n",
    "    \n",
    "    # Start/end date formatting\n",
    "    start_date_dict = {\"01\": year+'-01-01',\n",
    "                   \"02\": year+'-02-01',\n",
    "                   \"03\": year+'-03-01',\n",
    "                   \"04\": year+'-04-01',\n",
    "                   \"05\": year+'-05-01',\n",
    "                   \"06\": year+'-06-01',\n",
    "                   \"07\": year+'-07-01',\n",
    "                   \"08\": year+'-08-01',\n",
    "                   \"09\": year+'-09-01',\n",
    "                   \"10\": year+'-10-01',\n",
    "                   \"11\": year+'-11-01',\n",
    "                   \"12\": year+'-12-01'}\n",
    "    end_date_dict =   {\"01\": year+'-01-31',\n",
    "                   \"02\": year+'-02-28',\n",
    "                   \"03\": year+'-03-31',\n",
    "                   \"04\": year+'-04-30',\n",
    "                   \"05\": year+'-05-31',\n",
    "                   \"06\": year+'-06-30',\n",
    "                   \"07\": year+'-07-31',\n",
    "                   \"08\": year+'-08-31',\n",
    "                   \"09\": year+'-09-30',\n",
    "                   \"10\": year+'-10-31',\n",
    "                   \"11\": year+'-11-30',\n",
    "                   \"12\": year+'-12-31'}\n",
    "    \n",
    "    # Query catalog\n",
    "    site_cat = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=start_date_dict[month], \n",
    "        end_datetime=end_date_dict[month],\n",
    "        max_cloud_cover=100,\n",
    "        collections='sentinel2_l2a'\n",
    "        )\n",
    "    if len(site_cat) > 0:\n",
    "        site_cat = gpd.sjoin(geom, site_cat)\n",
    "        \n",
    "    return(site_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image Chips\n",
    "\n",
    "* Read and create image chips for 10km grid\n",
    "* Select highest quality chips per site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chips(site_cat, chip_size=300, unmsk_frac=0.75, repartition_size=partitions):\n",
    "    \n",
    "    # Reads in 20-m QA band to use in masking and selection of best scenes\n",
    "    # Filter out blank chips at edge of scenes\n",
    "    # Handle rare edge case where returned chip is less than specified size (when reach edge of a scene)\n",
    "    # Mask chips by QA band and compute count of unmasked cells\n",
    "    # Remove chips with less than a minimum fraction of unmasked cells    \n",
    "    chip_size_scl = int(chip_size/2)   \n",
    "    bad_scl_values = [0, 1, 2, 3, 8, 9, 10]\n",
    "    site_chips = spark.read.chip(site_cat, ['SCL_20m'],\n",
    "                                 chipping_strategy=chp.CentroidCentered(chip_size_scl)) \\\n",
    "                          .select('uid', 'id', 'SCL_20m') \\\n",
    "                          .withColumn('mask', rf_make_constant_tile(1, chip_size_scl, chip_size_scl, 'uint16')) \\\n",
    "                          .withColumn('tot_cell_count', rf_data_cells('SCL_20m')) \\\n",
    "                          .filter(pys.col('tot_cell_count') == chip_size_scl*chip_size_scl) \\\n",
    "                          .withColumn('mask', rf_mask_by_values('mask', 'SCL_20m', bad_scl_values)) \\\n",
    "                          .withColumn('unmsk_cell_count', rf_data_cells('mask')) \\\n",
    "                          .filter(pys.col('unmsk_cell_count') >= unmsk_frac*chip_size_scl*chip_size_scl) \\\n",
    "                          .repartition(repartition_size, 'uid', 'id')\n",
    "    \n",
    "    # Find the chip(s) with the highest number of unmasked cells\n",
    "    # If there's >1 chip (a tie) take the first record  \n",
    "    chpinf_pdf = site_chips.select('uid', 'id', 'unmsk_cell_count').toPandas()\n",
    "    chpinf_pdf['grpid'] = chpinf_pdf['uid']    \n",
    "    site_maxcnt = chpinf_pdf.sort_values('unmsk_cell_count', ascending=False) \\\n",
    "                            .groupby(['grpid']).first() \\\n",
    "                            .drop('unmsk_cell_count', axis=1)\n",
    "    \n",
    "    # Read in thermal band for highest quality chip\n",
    "    site_cat = site_cat.merge(site_maxcnt, on=['uid', 'id'], how='inner')\n",
    "    site_chip_unq = spark.read.chip(site_cat, ['B04_10m','B03_10m','B02_10m'],\n",
    "                                chipping_strategy=chp.CentroidCentered(chip_size)) \\\n",
    "                     .select('uid', 'site_type', 'id', 'datetime', 'B04_10m', 'B03_10m', 'B02_10m') \\\n",
    "                     .withColumn('Red', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B04_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                     .withColumn('Green', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B03_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                     .withColumn('Blue', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B02_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                     .drop('B04_10m', 'B03_10m', 'B02_10m') \\\n",
    "                     .repartition(repartition_size, 'uid')\n",
    "    \n",
    "    return(site_chip_unq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GeoTIFFs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(tif_filename, png_filename):\n",
    "    with rasterio.open(tif_filename) as infile:\n",
    "        \n",
    "        profile = infile.profile\n",
    "        profile['driver'] = 'PNG'\n",
    "        \n",
    "        raster = infile.read()\n",
    "        \n",
    "        with rasterio.open(png_filename, 'w', **profile) as dst:\n",
    "            dst.write(raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PNGs from RasterFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_from_rf(rf):\n",
    "    \n",
    "    # Delete temporary output paths for geotiffs and pngs and all files if they exist\n",
    "    if os.path.exists('geotiffs'):\n",
    "        shutil.rmtree('geotiffs')\n",
    "    if os.path.exists('pngs'):\n",
    "        shutil.rmtree('pngs')\n",
    "    \n",
    "    # Create GeoTIFFs from RasterFrame\n",
    "    rf.write.chip('geotiffs', filenameCol='file_path_name', catalog=False)\n",
    "    tif_file_list = glob.glob('geotiffs/*.tif')\n",
    "    \n",
    "    # Create output paths for PNGs to fit Fastai structure\n",
    "    os.mkdir('pngs')\n",
    "    os.mkdir('pngs/all')\n",
    "    png_file_list = [f.replace('.tif', '.png').replace('geotiffs/', 'pngs/all/') for f in tif_file_list]\n",
    "    \n",
    "    # Convert and write out PNGs\n",
    "    for i in range(0, len(tif_file_list)):\n",
    "        convert_image(tif_file_list[i], png_file_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Output Function\n",
    "\n",
    "* Writes out scores to GeoJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chip_scores(rf, pdf, year, month):\n",
    "    \n",
    "    # Get tile extents from RasterFrame\n",
    "    geo_pdf = rf.withColumn('geometry', st_reproject(st_geometry(rf_extent('Red')), \n",
    "                                                     rf_crs('Red'), \n",
    "                                                     pys.lit('EPSG:4326'))) \\\n",
    "                .select('uid', 'site_type', 'geometry').toPandas()\n",
    "    geo_pdf['year'] = year\n",
    "    geo_pdf['month'] = month\n",
    "    geo_gdf = gpd.GeoDataFrame(geo_pdf, geometry='geometry', crs='EPSG:4326')\n",
    "    \n",
    "    # Join with scores\n",
    "    scores_gdf = pd.merge(geo_gdf, pdf, how='inner', on=['uid', 'site_type'])\n",
    "    \n",
    "    scores_gdf.to_file(output_score_file, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Cement and Steel Plant sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cement_site_gdf = gpd.read_file(cement_site_geojson)\n",
    "cement_site_gdf['site_type'] = 'cement'\n",
    "steel_site_gdf = gpd.read_file(steel_site_geojson)\n",
    "steel_site_gdf['site_type'] = 'steel'\n",
    "chip_cntr_gdf = pd.concat([cement_site_gdf, steel_site_gdf], ignore_index=True)\n",
    "chip_cnt = len(chip_cntr_gdf)\n",
    "print(\"Total count of sites: \", chip_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of cement sites: \", len(chip_cntr_gdf[chip_cntr_gdf['site_type'] == 'cement']))\n",
    "print(\"Number of steel sites: \", len(chip_cntr_gdf[chip_cntr_gdf['site_type'] == 'steel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chips and Score\n",
    "\n",
    "For each site:\n",
    "\n",
    "* Get catalog of Sentinel-2 scenes that intersect with chip centroids\n",
    "* Read and create image chips\n",
    "* Score models\n",
    "* Write scores out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get catalog of Sentinel-2 scenes that intersect with chip centroids\n",
    "site_cat_yyyymm = eod_read_catalog(chip_cntr_gdf, year, month)\n",
    "    \n",
    "# Read and create image chips\n",
    "site_chip_yyyymm_unq = create_chips(site_cat_yyyymm, \n",
    "                                      chip_size=chip_size, \n",
    "                                      unmsk_frac=unmsk_frac,\n",
    "                                      repartition_size=round(chip_cnt/4))\n",
    "site_chip_yyyymm_unq = site_chip_yyyymm_unq.withColumn('file_path_name', \n",
    "                                                pys.concat_ws('_', pys.col('uid'), pys.col('site_type'))) \\\n",
    "                                           .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out temporary PNGs to score\n",
    "png_from_rf(site_chip_yyyymm_unq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score PNGs\n",
    "deployment_scores_pdf = score_pngs('pngs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write scores to GeoJSON file\n",
    "write_chip_scores(site_chip_yyyymm_unq, deployment_scores_pdf, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
