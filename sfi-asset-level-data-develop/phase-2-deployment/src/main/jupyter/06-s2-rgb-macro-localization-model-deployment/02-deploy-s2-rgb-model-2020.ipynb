{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment of Sentinel-2 RGB Macrolocalization Model - 2020\n",
    "\n",
    "This notebook deploys the Sentinel-2 RGB macrolocalization models for cement and steel plants for the year 2020.\n",
    "\n",
    "Deployement plan is as follows:\n",
    "\n",
    "1. Loop over each grid id.\n",
    "2. Select all scenes between May - August 2020.\n",
    "3. Choose the scene with least amount of cloud coverage, and score.\n",
    "4. Iterate over scenes ordered by cloud coverage, to fill in full deployment region intersecting with grid id.\n",
    "\n",
    "This idea is to reduce the amount of compute time required by reducing the complexing of how to find the best chip, and score whole scenes at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==1.0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthai.all import *\n",
    "import earthai.chipping.strategy as chp\n",
    "import pyspark.sql.functions as pys\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from shapely.wkt import loads\n",
    "import re\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "* Important to do this before defining the udfs for scoring\n",
    "* Set number of partitions on par with the number of catalog items per scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 250\n",
    "spark = create_earthai_spark_session(**{\n",
    "    \"spark.default.parallelism\": partitions,\n",
    "    \"spark.sql.shuffle.partitions\": partitions,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input/output files and paths, and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "* `chip_size` is the size of chips (length) to create (in pixels)\n",
    "* `year` defines the year of selected scenes; months restricted to May - August.\n",
    "* `scene_subset` set to 1 or 2. This divides the scoring in two pieces to run on two servers at the same time. 1 will process the first set of scenes; 2 will process the second. \n",
    "* `calc_crs` is a physical CRS used to compute distance from chip center and known plants that intersect.\n",
    "* `scale_size` slightly reduces the scene extent to avoid spending too much times scoring scene edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_size = 300 # 3 km for Sentinel-2\n",
    "year = '2020'\n",
    "scene_subset = 3\n",
    "calc_crs = \"EPSG:3395\"\n",
    "scale_size = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input files and paths\n",
    "\n",
    "* `s2_deployment_gjson` is the deployment region for the S2 RGB model; used to define region to score\n",
    "* `s2_scene_gjson` is the GeoJSON defining Sentinel-2 grid-ids to query; used to loop over scenes for deployment process\n",
    "* `s2_grid_gjson` is the 10km grid and prediction values from infrastructure density model; joined to results at the end to merge scores\n",
    "* `cement_plant_gjson` gives exact lat/long of cement plants; joined to results at the end to find chips intersection with known plants\n",
    "* `steel_plant_gjson` gives exact lat/long of steel plants; joined to results at the end to find chips intersection with known plants\n",
    "* `CEMENT_MODEL_PATH` is the path on S3 to the VGG13 multiclass model (best recall for cement: 89.1%)\n",
    "* `STEEL_MODEL_PATH` is the path on S3 to the DenseNet161 multiclass model (best recall for steel: 89.7%)\n",
    "* `LOCAL_DIR` specifies where to put files locally for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_deployment_gjson = '../../resources/macro-loc-model-deployment4/S2-deployment-region-CHN-10km-nowater.geojson'\n",
    "s2_scene_gjson = '../../resources/macro-loc-model-deployment4/S2-deployment-scene-extents-CHN-10km-nowater.geojson'\n",
    "\n",
    "s2_grid_gjson = '../../resources/macro-loc-model-deployment4/S2-deployment-grid-CHN-10km-nowater.geojson'\n",
    "cement_plant_gjson = '../../resources/macro-loc-model-build4/cement_exact_china_v4.1_s2.geojson'\n",
    "steel_plant_gjson = '../../resources/macro-loc-model-build4/steel_exact_china_v4.1_s2.geojson'\n",
    "\n",
    "CEMENT_MODEL_PATH = 'S2-RGB-macro-localization-model-build4/S2-RGB-model-results4/vgg13_multiclass_final.pkl'\n",
    "STEEL_MODEL_PATH = 'S2-RGB-macro-localization-model-build4/S2-RGB-model-results4/densenet161_multiclass_final.pkl'\n",
    "LOCAL_DIR = '/scratch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files and paths\n",
    "\n",
    "* `s3_path` defines S3 high-level folder for S2 RGB macro-localization deployment results\n",
    "* `output_path` defines (temporary) local place of storage\n",
    "* `output_score_tar` define output tar of score GeoJSONS (one for each scene)\n",
    "* `output_gjson_prefix` is the prefix for output GeoJSON files with scores for each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 'S2-RGB-macro-localization-model-deployment4'\n",
    "\n",
    "output_path = 'S2-deployment-chip-scores-CHN-10km-nowater-'+year+'-set'+str(scene_subset)\n",
    "output_score_tar = output_path+'.tar'\n",
    "output_gjson_prefix = 'S2-deployment-chip-scores-CHN-10km-nowater-'+year+'-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model and Define Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('sfi-shared-assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download models and load learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(MODEL_PATH):\n",
    "    if not os.path.exists(LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\")):\n",
    "        os.makedirs(LOCAL_DIR + MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "    bucket.download_file(MODEL_PATH, LOCAL_DIR+MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\") + \"/export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model(CEMENT_MODEL_PATH)\n",
    "download_model(STEEL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cement_model = load_learner(LOCAL_DIR+CEMENT_MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))\n",
    "steel_model = load_learner(LOCAL_DIR+STEEL_MODEL_PATH.split(\"/\")[-1].replace(\".pkl\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring function for PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pngs(path):\n",
    "    \n",
    "    # Get ImageDataBunch for Fastai\n",
    "    data = (ImageDataBunch.from_folder(path, train='all', bs=16, num_workers=0, seed=42).normalize(imagenet_stats))\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    data_cnt = len(data.train_ds)\n",
    "    scene_id = []\n",
    "    tile_id = []\n",
    "    cement_prob = []\n",
    "    steel_prob = []\n",
    "    \n",
    "    # Loop over images and get scores and metadata\n",
    "    for i in range(0, data_cnt):\n",
    "        \n",
    "        # Cement model probability\n",
    "        p_cement_model = cement_model.predict(data.train_ds.x[i])\n",
    "        cement_prob.append(to_np(p_cement_model[2])[0].item())\n",
    "    \n",
    "        # Steel model probability\n",
    "        p_steel_model = steel_model.predict(data.train_ds.x[i])\n",
    "        steel_prob.append(to_np(p_steel_model[2])[2].item())\n",
    "    \n",
    "        # Metadata for chip\n",
    "        scene_id.append('-'.join(str(data.items[i]).split('/')[-1].split('-')[0:2]))\n",
    "        tile_id.append(str(data.items[i]).split('/')[-1].split('.')[0])\n",
    "        \n",
    "    # Return data frame\n",
    "    score_pdf = pd.DataFrame({'s2_grid_id': scene_id,\n",
    "                              'tile_id': tile_id,\n",
    "                              'tile_cmt_prob': cement_prob,\n",
    "                              'tile_stl_prob': steel_prob})\n",
    "    \n",
    "    return(score_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EOD Catalog Read and Chipping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get catalog of Sentinel-2 scenes that intersect with deployment regions\n",
    "\n",
    "Queries EarthAI Catalog to find S2 scenes that intersect with deployment region.\n",
    "\n",
    "* Takes single grid_id; returns scenes/datetimes from May - August\n",
    "* Join back to deployment region for later clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scene_deployment_region(base_gdf, overlay_gdf):\n",
    "    \n",
    "    # Find interesection of the base GeoDataFrame and the Overlay GeoDataFrame\n",
    "    intersect_gdf = gpd.overlay(base_gdf, overlay_gdf, how='intersection')\n",
    "    \n",
    "    # Convert Polygons to Multipolygons\n",
    "    multipoly_geom = [MultiPolygon([geom]) if (geom.type == 'Polygon') else geom for geom in intersect_gdf.geometry]\n",
    "    intersect_gdf = intersect_gdf.set_geometry(multipoly_geom)\n",
    "    \n",
    "    # Change precision on lat/long to 0.0001 (~10m at equator)\n",
    "    simpledec = re.compile(r\"\\d*\\.\\d+\")\n",
    "    def mround(match):\n",
    "        return \"{:.4f}\".format(float(match.group()))\n",
    "    intersect_gdf.geometry = intersect_gdf.geometry.apply(lambda x: loads(re.sub(simpledec, mround, x.wkt)))\n",
    "    \n",
    "    return(intersect_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_region_difference(base_gdf, overlay_gdf):\n",
    "    \n",
    "    # Get simplied representation of scored region from chips extents\n",
    "    chip_geoms_union = overlay_gdf.unary_union\n",
    "    if isinstance(chip_geoms_union, Polygon) or isinstance(chip_geoms_union, MultiPolygon):\n",
    "        chip_geoms_union = [chip_geoms_union]\n",
    "        \n",
    "    chip_geoms_union = [MultiPolygon([x]) if (x.type == 'Polygon') else x for x in chip_geoms_union]\n",
    "    scored_region_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(chip_geoms_union),\n",
    "                                         crs='EPSG:4326')\n",
    "    \n",
    "    # Compute and return remaining region to score\n",
    "    difference_gdf = gpd.overlay(base_gdf, scored_region_gdf, how='difference')\n",
    "    diff_geom = [MultiPolygon([x]) if (x.type == 'Polygon') else x for x in difference_gdf.geometry]\n",
    "    difference_gdf.geometry = gpd.GeoSeries(diff_geom)\n",
    "    \n",
    "    return(difference_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eod_read_catalog(geom, grid_id, year, max_cc=100):\n",
    "    \n",
    "    # Start/end date formatting\n",
    "    start_date = year+'-05-01'\n",
    "    end_date = year+'-08-31'\n",
    "    \n",
    "    # Query catalog\n",
    "    site_cat = earth_ondemand.read_catalog(\n",
    "        geo=geom,\n",
    "        start_datetime=start_date,\n",
    "        end_datetime=end_date,\n",
    "        max_cloud_cover=max_cc,\n",
    "        collections='sentinel2_l2a',\n",
    "        grid_ids=[grid_id]\n",
    "        )\n",
    "    if len(site_cat) > 0:\n",
    "        \n",
    "        # Sort by cloud coverage and return\n",
    "        site_cat = site_cat.sort_values('eo_cloud_cover')\n",
    "        return(site_cat)\n",
    "        \n",
    "    else:\n",
    "        return([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image Chips\n",
    "\n",
    "* Read and create image chips for 10km grid\n",
    "* Select highest quality scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chips(site_cat, chip_size=300):\n",
    "    \n",
    "    # Uses scene aligned grid to create same-size chips\n",
    "    # Grabs red, green, and blue bands\n",
    "    # Filter out chips smaller than chip_size x chips_size\n",
    "    # Rename columns\n",
    "    # Normalize and convert data bands to uint16\n",
    "    w = Window().orderBy(lit('dummy'))\n",
    "    site_chip_filt = spark.read.chip(site_cat, ['B04_10m','B03_10m','B02_10m'],\n",
    "                                    chipping_strategy=chp.SceneAlignedGrid(chip_size,chip_size)) \\\n",
    "                         .select('eod_grid_id', 'id', 'datetime', 'eo_cloud_cover', \n",
    "                                 'B04_10m', 'B03_10m', 'B02_10m') \\\n",
    "                         .withColumn('tile_dims', rf_dimensions('B04_10m')) \\\n",
    "                         .filter((pys.col('tile_dims').rows == chip_size) & \n",
    "                                 (pys.col('tile_dims').cols == chip_size)) \\\n",
    "                         .withColumn('Red_uint16', rf_convert_cell_type('B04_10m', 'uint16')) \\\n",
    "                         .withColumn('nodata_cell_cnt', rf_no_data_cells('Red_uint16')) \\\n",
    "                         .filter(pys.col('nodata_cell_cnt') == 0).cache()\n",
    "    isNotEmptyRF = len(site_chip_filt.head(1))\n",
    "    \n",
    "    if isNotEmptyRF == 1:\n",
    "        site_chip_unq = site_chip_filt.withColumnRenamed('eod_grid_id', 's2_grid_id') \\\n",
    "                         .withColumnRenamed('eo_cloud_cover', 's2_eo_cloud_cover') \\\n",
    "                         .withColumnRenamed('id', 's2_id') \\\n",
    "                         .withColumnRenamed('datetime', 's2_datetime') \\\n",
    "                         .withColumn('tile_id', pys.concat_ws('-',\n",
    "                                                              pys.col('s2_grid_id'), \n",
    "                                                              pys.lpad(pys.row_number().over(w), 4, '0'))) \\\n",
    "                         .withColumn('Red', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B04_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .withColumn('Green', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B03_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .withColumn('Blue', \n",
    "                                 rf_convert_cell_type(\n",
    "                                     rf_local_multiply(\n",
    "                                         rf_rescale(rf_convert_cell_type('B02_10m', 'uint16')), 65535), 'uint16')) \\\n",
    "                         .drop('B04_10m', 'B03_10m', 'B02_10m', 'tile_dims', 'Red_uint16', 'nodata_cell_cnt') \\\n",
    "                         .cache()\n",
    "    \n",
    "        return(site_chip_unq)\n",
    "    \n",
    "    else:\n",
    "        return(site_chip_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GeoTIFFs to PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(tif_filename, png_filename):\n",
    "    with rasterio.open(tif_filename) as infile:\n",
    "        \n",
    "        profile = infile.profile\n",
    "        profile['driver'] = 'PNG'\n",
    "        \n",
    "        raster = infile.read()\n",
    "        \n",
    "        with rasterio.open(png_filename, 'w', **profile) as dst:\n",
    "            dst.write(raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PNGs from RasterFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_from_rf(rf):\n",
    "    \n",
    "    # Create GeoTIFFs from RasterFrame\n",
    "    rf.write.chip('geotiffs', filenameCol='tile_id', catalog=False)\n",
    "    tif_file_list = glob.glob('geotiffs/*.tif')\n",
    "    \n",
    "    # Create output paths for PNGs to fit Fastai structure\n",
    "    os.mkdir('pngs')\n",
    "    os.mkdir('pngs/all')\n",
    "    png_file_list = [f.replace('.tif', '.png').replace('geotiffs/', 'pngs/all/') for f in tif_file_list]\n",
    "    \n",
    "    # Convert and write out PNGs\n",
    "    for i in range(0, len(tif_file_list)):\n",
    "        convert_image(tif_file_list[i], png_file_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Output Function\n",
    "\n",
    "* Writes out scores to GeoJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_chip_scores(rf, pdf, year, scene_grp):\n",
    "\n",
    "    # Get tile geometries from RasterFrame\n",
    "    geo_pdf = rf.withColumn('crs', rf_crs('Red')) \\\n",
    "                .withColumn('geometry', rf_geometry('Red')) \\\n",
    "                .select('tile_id', 's2_grid_id', 's2_id', 's2_datetime', 's2_eo_cloud_cover', \n",
    "                        'crs', 'geometry') \\\n",
    "                .toPandas()\n",
    "    \n",
    "    # Join with scores and create GeoDataFrame\n",
    "    scores_pdf = pd.merge(geo_pdf, deployment_scores_pdf, \n",
    "                          how='inner', on=['tile_id', 's2_grid_id'])\n",
    "    scores_gdf = gpd.GeoDataFrame(scores_pdf,\n",
    "                                  geometry='geometry',\n",
    "                                  crs=scores_pdf.crs[0].crsProj4) \\\n",
    "                    .drop('crs', axis=1)\n",
    "    \n",
    "    # Convert crs\n",
    "    scores_gdf = scores_gdf.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Update tile_id to include year and scene_grp\n",
    "    scores_gdf.tile_id = [x+'-'+year+'-'+str(scene_grp).zfill(2) for x in scores_gdf.tile_id]\n",
    "    \n",
    "    # Return joined GeoDataFrame\n",
    "    return(scores_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_inds_aggs(idf):\n",
    "    \n",
    "    # Includes area-weighted pred and max pred for cement and steel\n",
    "    # Create list of indexes that intersect with chip\n",
    "    odf = {}\n",
    "    odf['tile_inds_cmt_pred_wavg'] = sum(idf.inds_cmt_pred*idf.area) / sum(idf.area)\n",
    "    odf['tile_inds_stl_pred_wavg'] = sum(idf.inds_stl_pred*idf.area) / sum(idf.area)\n",
    "    odf['tile_inds_cmt_pred_max'] = max(idf.inds_cmt_pred)\n",
    "    odf['tile_inds_stl_pred_max'] = max(idf.inds_stl_pred)\n",
    "    odf['tile_inds_ids'] = (',').join(str(x) for x in idf.inds_id.tolist())\n",
    "    \n",
    "    return pd.Series(odf, index=['tile_inds_ids', 'tile_inds_cmt_pred_wavg', 'tile_inds_stl_pred_wavg',\n",
    "                                 'tile_inds_cmt_pred_max', 'tile_inds_stl_pred_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_inds_info(base_gdf, inds_gdf):\n",
    "    \n",
    "    # Find intersection of base_gdf and inds_gdf and compute area\n",
    "    # of the geometry\n",
    "    insct_inds_gdf = gpd.overlay(base_gdf, inds_gdf, how='intersection')\n",
    "    insct_inds_gdf = insct_inds_gdf.to_crs(calc_crs)\n",
    "    insct_inds_gdf['area'] = insct_inds_gdf.area\n",
    "    \n",
    "    # Compute aggregations on inds_gdf by tile_id\n",
    "    agg_inds_gdf = insct_inds_gdf.groupby('tile_id') \\\n",
    "                                 .apply(calc_inds_aggs) \\\n",
    "                                 .reset_index()\n",
    "    \n",
    "    # Join back to base_gdf and return\n",
    "    output_gdf = pd.merge(base_gdf, agg_inds_gdf, how='left', on='tile_id')\n",
    "    return(output_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_known_plants(base_gdf, plant_gdf):\n",
    "    \n",
    "    # uid column name (cmt or stl)\n",
    "    uid_cname = plant_gdf.columns.tolist()[0]\n",
    "    plnt_label = uid_cname.split('_')[0]+'_'\n",
    "    \n",
    "    # Spatial join base_gdf to plant_gdf to find where\n",
    "    # plants are within tiles\n",
    "    tile_gdf = base_gdf[['tile_id','geometry']]\n",
    "    tile_gdf = gpd.sjoin(tile_gdf, plant_gdf, how='inner', op='intersects') \\\n",
    "                  .sort_values(uid_cname) \\\n",
    "                  .drop('index_right', axis=1) \\\n",
    "                  .reset_index(drop=True)\n",
    "    \n",
    "    # Proceed if find plants\n",
    "    if len(tile_gdf) > 0:\n",
    "        \n",
    "        # Convert to physical crs\n",
    "        tile_gdf['geometry'] = tile_gdf.geometry.centroid\n",
    "        tile_gdf = tile_gdf.to_crs(calc_crs)\n",
    "        \n",
    "        # Get intersecting plants and convert to physical crs\n",
    "        plant_phys_gdf = plant_gdf[plant_gdf[uid_cname].isin(tile_gdf[uid_cname].tolist())]\n",
    "        plant_phys_gdf = plant_phys_gdf.reset_index(drop=True)\n",
    "        plant_phys_gdf = plant_phys_gdf.to_crs(calc_crs)\n",
    "        \n",
    "        # Make sure id's all lined up\n",
    "        vals_match = (tile_gdf[uid_cname] == plant_phys_gdf[uid_cname])\n",
    "        if sum(vals_match) != len(vals_match):\n",
    "            sys.exit('ERROR: Spatial join between tile and known plants is misaligned; cannot compute distance')\n",
    "            \n",
    "        # Calculate distance between chip center and plant\n",
    "        tile_gdf['tile_'+plnt_label+'distm'] = tile_gdf.distance(plant_phys_gdf)\n",
    "        \n",
    "        # Join back to base_gdf\n",
    "        merged_gdf = pd.merge(base_gdf, \n",
    "                              tile_gdf.drop('geometry', axis=1),\n",
    "                              how='left', on='tile_id')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Add columns of NaNs\n",
    "        merged_gdf = base_gdf\n",
    "        merged_gdf[uid_cname] = np.nan\n",
    "        merged_gdf['tile_'+plnt_label+'distm'] = np.nan\n",
    "        \n",
    "    return(merged_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chip_scores(output_gdf, year, scene_grp):\n",
    "\n",
    "    # Write output to GeoJson\n",
    "    output_score_file = output_path+'/'+output_gjson_prefix+output_gdf.s2_grid_id[0]+'-sg'+str(scene_grp).zfill(2)+'.geojson'\n",
    "    output_gdf.to_file(output_score_file, driver='GeoJSON')\n",
    "    \n",
    "    # Return name of score file\n",
    "    return(output_score_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Input Geometries for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Region and S2 scene list from 10km Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_deployment_gdf = gpd.read_file(s2_deployment_gjson)\n",
    "s2_scene_gdf = gpd.read_file(s2_scene_gjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10km Grid with Predictions from Infrastructure Density Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_grid_gdf = gpd.read_file(s2_grid_gjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Known cement and steel plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_plant_gdf = gpd.read_file(cement_plant_gjson)\n",
    "cmt_plant_gdf = cmt_plant_gdf[['uid', 'geometry']].rename(columns={'uid': 'cmtv4p1_uid'}) \\\n",
    "                                                  .sort_values('cmtv4p1_uid') \\\n",
    "                                                  .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_plant_gdf = gpd.read_file(steel_plant_gjson)\n",
    "stl_plant_gdf = stl_plant_gdf[['uid', 'geometry']].rename(columns={'uid': 'stlv4p1_uid'}) \\\n",
    "                                                  .sort_values('stlv4p1_uid') \\\n",
    "                                                  .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split scoring effort in four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_scene_gdf['grid_int'] = [int(n.split('-')[1][0:2]) for n in s2_scene_gdf.grid_id.tolist()]\n",
    "if scene_subset == 1:\n",
    "    s2_scene_gdf = s2_scene_gdf[s2_scene_gdf.grid_int <= 48]\n",
    "if scene_subset == 2:    \n",
    "    s2_scene_gdf = s2_scene_gdf[(s2_scene_gdf.grid_int > 48) & (s2_scene_gdf.grid_id < 'MGRS-50QLK')]\n",
    "if scene_subset == 3:\n",
    "    s2_scene_gdf = s2_scene_gdf[s2_scene_gdf.grid_id >= 'MGRS-50QLK']\n",
    "    s2_scene_gdf = s2_scene_gdf.iloc[0:200]\n",
    "if scene_subset == 4:\n",
    "    s2_scene_gdf = s2_scene_gdf[s2_scene_gdf.grid_id >= 'MGRS-50QLK']\n",
    "    s2_scene_gdf = s2_scene_gdf.iloc[200:]\n",
    "scene_ids = s2_scene_gdf.grid_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scene_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fail-safe\n",
    "\n",
    "If server crashes, this picks up where we left off, so don't have to rerun scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_scene_list = os.listdir(output_path)\n",
    "scored_scene_list.sort()\n",
    "if len(scored_scene_list) > 0:\n",
    "    last_scored_scene = ('-').join(scored_scene_list[-1].split('.')[0].split('-')[8:10])\n",
    "    last_ind = scene_ids.index(last_scored_scene)\n",
    "    # Repeat the last scene, in case didn't fully finish\n",
    "    scene_ids = scene_ids[last_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scene_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary code to score specific scenes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scene_ids = ['MGRS-48RXP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over Scenes, Create Chips, and Score\n",
    "\n",
    "For each scene:\n",
    "\n",
    "* Get catalog of Sentinel-2 scenes between May and August of specified year that intersect with deployment region\n",
    "* Read in scenes to create image chips\n",
    "* Read in QA?\n",
    "* Clip to deployment region\n",
    "* Score models\n",
    "* Write scores out to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary output paths for geotiffs and pngs and all files if they exist\n",
    "if os.path.exists('geotiffs'):\n",
    "    shutil.rmtree('geotiffs')\n",
    "if os.path.exists('pngs'):\n",
    "    shutil.rmtree('pngs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all scenes to score:\n",
    "for scene_id in scene_ids:\n",
    "    \n",
    "    # Time tracking:\n",
    "    stime = time.time()\n",
    "    \n",
    "    # Clean up output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Feedback on progress\n",
    "    print('Scoring Scene '+scene_id+' ('+str(scene_ids.index(scene_id)+1)+'/'+str(len(scene_ids))+')...')\n",
    "    \n",
    "    # Step 1: initilization of scene's deployment region\n",
    "    #         (intersection of scene extent and deployment region)\n",
    "    print('.....Computing intersection of scene with deployment region'+'...')\n",
    "    scene_full_gdf = s2_scene_gdf[s2_scene_gdf.grid_id == scene_id]\n",
    "    \n",
    "    # Scale scene extent down a bit to avoid scoring the edges only\n",
    "    scene_scaled_gdf = gpd.GeoDataFrame(geometry=scene_full_gdf.scale(scale_size, scale_size),\n",
    "                                        crs='EPSG:4326')\n",
    "    deployment_region_n_gdf = calc_scene_deployment_region(scene_scaled_gdf, \n",
    "                                                           macro_deployment_gdf)\n",
    "    deployment_region_n_gdf = deployment_region_n_gdf[deployment_region_n_gdf.geometry.is_valid]\n",
    "    \n",
    "    # To keep track of number of chips scored for each scene\n",
    "    tot_chp_scored = 0\n",
    "    \n",
    "    # Proceed if deployment region is not empty:\n",
    "    if len(deployment_region_n_gdf) > 0:\n",
    "    \n",
    "        # Step 2: Find all scenes from May - August of specified year for specified\n",
    "        #         scene_id; no limit on maximum cloud coverage\n",
    "        print('.....Finding all scenes from May - August '+year+'...')\n",
    "        site_cat_all = eod_read_catalog(deployment_region_n_gdf, scene_id, year, max_cc=100)\n",
    "        scene_cnt = len(site_cat_all)\n",
    "    \n",
    "        # Proceed to next step if query returns results\n",
    "        if scene_cnt > 0:\n",
    "        \n",
    "            # Loop over all returned scenes, in ascending order of cloud coverage\n",
    "            scene_grp = list(range(0, scene_cnt))\n",
    "            for n in scene_grp:\n",
    "            \n",
    "                # Feedback on progress\n",
    "                print('.....Creating chips for group '+str(n+1)+'...')\n",
    "    \n",
    "                # Step 3: get scene with (n+1)th least cloud coverage\n",
    "                #         and create chips\n",
    "                site_cat = site_cat_all.iloc[[n]]\n",
    "                site_cat = gpd.sjoin(deployment_region_n_gdf, site_cat)\n",
    "                if len(site_cat) > 0:\n",
    "                    site_chips = create_chips(site_cat, chip_size=chip_size)\n",
    "                    isNotEmptyRF = len(site_chips.head(1))\n",
    "                else:\n",
    "                    isNotEmptyRF = 0\n",
    "    \n",
    "                # Proceed to next step if able to create chips\n",
    "                if isNotEmptyRF == 1:\n",
    "                \n",
    "                    # Feedback on progress\n",
    "                    print('.....Creating geotiffs and pngs for group '+str(n+1)+'...')\n",
    "                \n",
    "                    # Step 4: create pngs to score\n",
    "                    png_from_rf(site_chips)\n",
    "        \n",
    "                    # Proceed to next step if pngs successfully created\n",
    "                    if len(glob.glob('pngs/all/*.png')) > 0:\n",
    "                    \n",
    "                        # Feed back on progress\n",
    "                        print('.....Scoring model for group '+str(n+1)+'...')\n",
    "                    \n",
    "                        # Step 5: score model and join to RasterFrame to create output\n",
    "                        deployment_scores_pdf = score_pngs('pngs')\n",
    "                        output_gdf = join_chip_scores(site_chips, deployment_scores_pdf, year, n+1)\n",
    "                    \n",
    "                        # Clean up temporary files\n",
    "                        if os.path.exists('geotiffs'):\n",
    "                            shutil.rmtree('geotiffs')\n",
    "                        if os.path.exists('pngs'):\n",
    "                            shutil.rmtree('pngs')\n",
    "                        \n",
    "                        # Step 6: merge output with results from infrastructure density model\n",
    "                        print('.....Joining Sentinel-2 scores with infrastructure density model for group '+str(n+1)+'...')\n",
    "                        output_gdf = merge_inds_info(output_gdf, s2_grid_gdf)\n",
    "                        \n",
    "                        # Step 7: merge output with known plants\n",
    "                        print('.....Joining Sentinel-2 scores with known plants for group '+str(n+1)+'...')\n",
    "                        output_gdf = merge_known_plants(output_gdf, cmt_plant_gdf)\n",
    "                        output_gdf = merge_known_plants(output_gdf, stl_plant_gdf)\n",
    "                    \n",
    "                        # Step 8: write results out to GeoJSON\n",
    "                        print('.....Writing scores to GeoJSON for group '+str(n+1)+'...')\n",
    "                        oscore_file = write_chip_scores(output_gdf, year, n+1)\n",
    "                        grp_chp_scored = len(output_gdf)\n",
    "                        tot_chp_scored = tot_chp_scored + grp_chp_scored\n",
    "                        print('.....Scored '+str(grp_chp_scored)+' chips in group '+str(n+1)+'...')\n",
    "            \n",
    "                        # Tidy up for next round\n",
    "                        if os.path.exists('geotiffs'):\n",
    "                            shutil.rmtree('geotiffs')\n",
    "                        if os.path.exists('pngs'):\n",
    "                            shutil.rmtree('pngs')\n",
    "            \n",
    "                        # Step 9: Compute remaining region left to score\n",
    "                        print('.....Calculating remaining region to score...')\n",
    "                        deployment_region_n_gdf = calc_region_difference(deployment_region_n_gdf,\n",
    "                                                                     output_gdf)\n",
    "                        deployment_region_n_gdf = deployment_region_n_gdf[deployment_region_n_gdf.geometry.is_valid]\n",
    "                    \n",
    "                        # Stop if no region left to score\n",
    "                        if len(deployment_region_n_gdf) == 0:\n",
    "                            print('.....Finished scoring scene '+scene_id+'.')\n",
    "                            break\n",
    "                        \n",
    "                    # Feedback if no pngs created\n",
    "                    else:\n",
    "                        print('.....Unable to create pngs for group '+str(n+1)+': 0 tiles scored.')\n",
    "            \n",
    "                # Feedback if no chips created\n",
    "                else:\n",
    "                    print('.....Unable to create chips for group '+str(n+1)+': 0 tiles scored.')\n",
    "        \n",
    "        # Feedback if query returns no results\n",
    "        else:\n",
    "            print('.....EOD query did not return results for scene '+scene_id+': 0 tiles scored.')\n",
    "    \n",
    "    # Feedback is original deployment region is empty\n",
    "    else:\n",
    "        print('.....Scene '+scene_id+' does not intersect with deployment region: 0 tiles scored.')\n",
    "        \n",
    "    # Time tracking:\n",
    "    etime = time.time()\n",
    "    print('...Total number of chips scored for '+scene_id+': '+str(tot_chp_scored)+'.')\n",
    "    print('...Total execution time for '+scene_id+': '+str((etime-stime)/60.)+' min.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tar results and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_code = 'tar -cvf '+output_score_tar+' '+output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(unix_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.upload_file(output_score_tar, \n",
    "                   s3_path+'/'+output_score_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EarthAI Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
